# Data

## Acquisition and Analysis

Write scripts in your favourite language to perform the following tasks:

1. Process a Wiktionary dump to extract synonym relations for a random language (not English, Ukrainian or Russian :). You can find the latest dumps at <https://dumps.wikimedia.org/backup-index.html>. The task requires application of XML SAX parsing.

2. Download and extract as separate texts all posts in a section of choice from <http://forum.lvivport.com>. The task requires web scraping.

3. Write a query to collect all relations from dbpedia for every individual person listed in it. The task requires running a SPARQL request at <https://dbpedia.org/sparql>.

4. Download and process an arbitrary file from Common Crawl (<https://index.commoncrawl.org/>), extract individual items, perform basic statistical analysis (distribution of hosts, words, languages, domains etc.) and visualization (optional).

## Annotation

1. According to the [annotation guidelines](https://github.com/lang-uk/ner-uk/blob/master/doc/README.md), annotate 2 documents from the BrUK corpus for Named Entities for your username at <http://ann.lisp.kiev.ua/>.

2. Process data from the [NUCLE Error Corpus](http://www.comp.nus.edu.sg/~nlp/conll14st.html#nucle32) and analyze inter-annotator agreement in it (general and for each error type). You can use only [the test data](https://github.com/andabi/deep-text-corrector/tree/master/data/conll14st-test-data) for this task.

## Project

1. Perform initial data collection for your project.
2. Devise and describe a way to collect data for your course project using crowdsourcing or from the users. Implement a proof-of-concept.


## Grading

Acquisition: 60%

- task1: 15%
- task2: 15%
- task3: 10%
- task4: 20%

Annotation: 30%

- task1: 10%
- task2: 20%

Project: 10%

## Deadline

23.03.2019
