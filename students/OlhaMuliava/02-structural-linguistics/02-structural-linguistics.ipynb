{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заголовки новин\n",
    "\n",
    "### 1. Форматування\n",
    "\n",
    "[The Associated Press Stylebook](https://www.amazon.com/Associated-Press-Stylebook-2017-Briefing/dp/0465093043/) - це посібник зі стилю, яким часто послуговуються журналісти по всьому світу. Він рекомендує такі правила форматування заголовків:\n",
    "1. З великої літери потрібно писати іменники, займенники, дієслова, прикметники, прислівники та підрядні сполучники. Якщо слово написане через дефіс, велику літеру потрібно додати для кожної частинки слова (наприклад, правильно \"Self-Reflection\", а не \"Self-reflection\").\n",
    "2. З великої літери потрібно писати перше і останнє слово заголовку, незалежно від частини мови.\n",
    "3. З маленької літери потрібно писати всі інші частини мови: артиклі/визначники, сурядні сполучники, прийменники, частки, вигуки.\n",
    "\n",
    "**Завдання:**\n",
    "1. напишіть програму, яка форматує заголовки за вказаними правилами\n",
    "2. проженіть вашу програму на [корпусі заголовків з The Examiner](examiner-headlines.txt)\n",
    "3. збережіть програму та файл із відформатованими заголовками у директорії з вашим іменем\n",
    "4. скільки заголовків у корпусі було відформатовано правильно? (скільки заголовків залишились незмінними?)\n",
    "\n",
    "Зверніть увагу, що ваша програма повинна правильно розрізняти прийменники та підрядні сполучники. Наприклад, `Do as you want` => `Do As You Want` (бо \"as\" тут є сполучником), but `How to use a Macbook as a table` => `How to Use a Macbook as a Table` (бо \"as\" тут є прийменником)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def style_capitalize(token):\n",
    "    if token.isupper():\n",
    "        return token\n",
    "    elif '-' in token:\n",
    "        parts = token.split('-')\n",
    "        capit_parts = [p.capitalize() if not p.isupper() else p for p in parts]\n",
    "        return '-'.join(capit_parts)\n",
    "    else:\n",
    "        return token.capitalize()\n",
    "\n",
    "def join_tokens(token_idx_list):\n",
    "    string = ''\n",
    "    prev_idx = 0\n",
    "    prev_token_len = 0\n",
    "    for  token, idx in token_idx_list:\n",
    "        string += ' '*(idx - prev_idx - prev_token_len) + token\n",
    "        prev_idx = idx\n",
    "        prev_token_len = len(token)\n",
    "        \n",
    "    return string\n",
    "    \n",
    "def press_style(string):\n",
    "    tokens = nlp(string)\n",
    "    \n",
    "    if tokens[-1].text == '\\n':\n",
    "        tokens = tokens[:-1]\n",
    "        \n",
    "    capit_tokens = []\n",
    "    for token in tokens:\n",
    "        if (token.i == 0) or (token.i == len(tokens) - 1):\n",
    "            capit_tokens.append((style_capitalize(token.text), token.idx))\n",
    "        elif token.pos_ in set(['NOUN', 'PROPN', 'PRON', 'VERB', 'ADJ', 'ADV']):\n",
    "            capit_tokens.append((style_capitalize(token.text), token.idx))\n",
    "        elif token.pos_ == 'ADP' and token.dep_ == 'mark':\n",
    "            capit_tokens.append((style_capitalize(token.text), token.idx))                   \n",
    "        else:\n",
    "            capit_tokens.append((token.text, token.idx))\n",
    "    \n",
    "    return join_tokens(capit_tokens) + '\\n'\n",
    "\n",
    "input_file = open(\"examiner-headlines.txt\", \"r\")\n",
    "lines = input_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"output.txt\", \"w+\")\n",
    "with open(\"output.txt\", 'a') as f:\n",
    "    for line in lines:\n",
    "        f.write(press_style(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Вірусні новини\n",
    "\n",
    "У статті [Automatic Extraction of News Values from Headline Text](http://www.aclweb.org/anthology/E17-4007) описано основні ознаки заголовків, які кидаються в очі і змушують читача таки прочитати новину:\n",
    "1. наявність імен людей, назв компаній тощо\n",
    "2. емоційне забарвлення\n",
    "3. ступені порівняння\n",
    "4. близькість\n",
    "5. елемент несподіванки\n",
    "6. унікальність\n",
    "\n",
    "**Завдання:**\n",
    "1. Напишіть програму, яка аналізує заголовок за першими трьома ознаками (у спрощеній формі)\n",
    "   * Чи є в заголовку іменовані стуності?\n",
    "   * Чи є заголовок позитивно чи негативно забарвлений?\n",
    "   * Чи є в заголовку прикметники та прислівники вищого і найвищого ступенів порівняння?\n",
    "2. Проженіть вашу програму на [корпусі заголовків з The Examiner](examiner-headlines.txt). Для кожної з трьох ознак, визначте відсоток заголовків у корпусі, які її мають.\n",
    "3. Збережіть програму та пораховану статистику в директорії з вашим іменем.\n",
    "\n",
    "Додаткова інформація:\n",
    "- Типи сутностей, які впливають на \"вірусність\" заголовка, виберіть самостійно.\n",
    "- Для визначення емоційного забарвлення, використайте [SentiWordNet](http://sentiwordnet.isti.cnr.it/). Наприклад, можна перевірити, що середнє значення позитивності/негативності слова у заголовку перевищує 0.5. Для визначення середнього значення можна брати до п'яти перших значень слова з такою частиною мови. Будьте креативними та експериментуйте.\n",
    "\n",
    "### Джерела\n",
    "\n",
    "Ви можете використати будь-яку мову програмування та будь-яку NLP-бібліотеку.\n",
    "\n",
    "Набір заголовків взятий із https://www.kaggle.com/therohk/examine-the-examiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn #, SentiSynset\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "def spacy_pos_to_wn(pos):\n",
    "    if pos in set(['NOUN', 'PROPN']):\n",
    "        return 'n'\n",
    "    elif pos.startswith('ADJ'):\n",
    "        return 'a'\n",
    "    elif pos.startswith('VERB'):\n",
    "        return 'v'\n",
    "    elif pos.startswith('ADV'):\n",
    "        return 'r'\n",
    "    return None\n",
    "\n",
    "def spam_title(string):    \n",
    "    tokens = nlp(string)\n",
    "    \n",
    "    if tokens[-1].text == '\\n':\n",
    "        tokens = tokens[:-1]\n",
    "    \n",
    "    sentiments = []\n",
    "    named_ent_bool = False\n",
    "    comp_superlative_bool = False\n",
    "    target_tags=['JJR', 'JJS', 'RBR', 'RBS']\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Named Entities\n",
    "        if token.ent_type_ in set(['PERSON', 'ORG', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'DATE', 'MONEY']):\n",
    "            named_ent_bool = True\n",
    "        \n",
    "        # Sentiment\n",
    "        wn_pos = spacy_pos_to_wn(token.pos_)\n",
    "        if wn_pos:\n",
    "            syns = list(swn.senti_synsets(token.text, pos=wn_pos)) \n",
    "            if syns:\n",
    "                # Calculate word sentiment as a mean value of the first 3 sentiments\n",
    "                syns_sentiments = [s.pos_score() - s.neg_score() for s in syns[0:4]]\n",
    "                sentiment = np.asarray(syns_sentiments).mean()\n",
    "                sentiments.append(sentiment)\n",
    "           \n",
    "        # Comparative / Superlative\n",
    "        if token.tag_ in target_tags:\n",
    "            comp_superlative_bool = True\n",
    "    \n",
    "    named_ents = int(named_ent_bool)\n",
    "    \n",
    "    \n",
    "    sentiment_num = np.asarray(sentiments).mean()\n",
    "    sentiment = 0 if abs(sentiment_num) <= 0.5 else 1\n",
    "    \n",
    "    comp_superlative = int(comp_superlative_bool)\n",
    "    \n",
    "    return named_ents, sentiment, comp_superlative\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 0)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_title('Halep slow enters Rogers Cup final in straight sets win over Errani')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = open(\"examiner-headlines.txt\", \"r\")\n",
    "lines = input_file.readlines()\n",
    "\n",
    "titles_spam_analysis = []\n",
    "for l in lines:\n",
    "    named_ents, sentiment, comp_superlative = spam_title(l)\n",
    "    titles_spam_analysis.append([named_ents, sentiment, comp_superlative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6886, 0.0028, 0.0448])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_all = np.asarray(titles_spam_analysis)\n",
    "np_stat = np_all.sum(axis=0) / np_all.shape[0]\n",
    "np_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"spam_output.txt\", \"w+\")\n",
    "with open(\"spam_output.txt\", 'a') as f:\n",
    "    f.write('named_ents, sentiment, comp_superlative\\n')\n",
    "    f.write(', '.join([str(i) for i in list(np_stat)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
