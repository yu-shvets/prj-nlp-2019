{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = '/Users/aromanov/Documents/Corpus/HTML/NYT/2008/07'\n",
    "files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(dir_name):\n",
    "    files += [os.path.join(dirpath, name) for name in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files = 6434\n",
      " 6434"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "\n",
    "print('Total files =', len(files))\n",
    "for i, file in enumerate(files[:]):\n",
    "    print('\\r', i+1, end='')\n",
    "    if file.endswith('.html.gz'):\n",
    "        with gzip.open(file) as gz:\n",
    "            soup = BeautifulSoup(gz, 'lxml')\n",
    "            text = ' '.join([p.text for p in soup.article.find_all('p', attrs={'class': 'story-content'})])\n",
    "            text = text.replace('“', '\"').replace('”', '\"')\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            sentences = [sentence for sentence in doc.sents]\n",
    "            i = 0\n",
    "            while i < len(sentences):\n",
    "                sentence = sentences[i]\n",
    "                if len(sentence) > 2:\n",
    "                    chunk = [[token.text, False] for token in sentence]\n",
    "                    # glue every two sentences to get enough positive samples for training\n",
    "                    if sentence[-1].text == '.' and i < len(sentences)-1:\n",
    "                        chunk.pop()\n",
    "                        chunk[-1][1] = True\n",
    "                        chunk2 = [[token.text, False] for token in sentences[i+1]]\n",
    "                        # 50% chance for lower case of next sent.\n",
    "                        if randint(0,1): chunk2[0][0] = chunk2[0][0].lower()\n",
    "                        chunk.extend(chunk2)\n",
    "                        i += 1\n",
    "                    train.append(chunk)\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('run-on-train-sm.json.gz', 'wt+') as fw:\n",
    "    json.dump(train, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
