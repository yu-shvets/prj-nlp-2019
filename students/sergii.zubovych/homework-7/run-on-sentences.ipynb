{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Завантажуємо спочатку тестові данні."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  273k  100  273k    0     0   443k      0 --:--:-- --:--:-- --:--:--  443k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/vseloved/prj-nlp-2019/master/tasks/07-language-as-sequence/run-on-test.json --output run-on-test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('run-on-test.json') as f:\n",
    "    run_on_js = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набираємо тренувальних данних з с корпусів NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import abc\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "source = []\n",
    "\n",
    "for fileid in reuters.fileids():\n",
    "    source.append(reuters.sents(fileid))\n",
    "    \n",
    "for fileid in gutenberg.fileids():\n",
    "    source.append(gutenberg.sents(fileid))    \n",
    "    \n",
    "for fileid in brown.fileids():\n",
    "    source.append(brown.sents(fileid))    \n",
    "    \n",
    "for fileid in abc.fileids():\n",
    "    source.append(abc.sents(fileid))        \n",
    "    \n",
    "for fileid in treebank.fileids():\n",
    "    source.append(treebank.sents(fileid))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далі генеруємо фейкові run-on речення на основі зібраного корпусу і розставляємо теги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', disable = ['ner','parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "# визначає випадковим способом скільки реченнь склеювати, \n",
    "# пропорції приблизні як у наданних вище тестових данних\n",
    "def rand_run_on_num():\n",
    "    x = random.random()\n",
    "    if x < 0.25:\n",
    "        return 0\n",
    "    elif 0.25 <= x < 0.97:\n",
    "        return 1\n",
    "    \n",
    "    return 2\n",
    "\n",
    "Token = namedtuple('Token', 'text point_after tag')\n",
    "\n",
    "random.seed(273757)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "def tag_tokens(tokens):   \n",
    "    \n",
    "    doc = nlp(' '.join(tokens))\n",
    "    \n",
    "    return [Token(token.text, False, token.tag_) for token in doc]\n",
    "\n",
    "\n",
    "for sents in source:        \n",
    "    \n",
    "    run_on_num = rand_run_on_num()\n",
    "    buff = []\n",
    "    prev_offset = 0   \n",
    "    \n",
    "    for sent in sents[1:]:\n",
    "        if len(sent) < 5:\n",
    "            continue\n",
    "\n",
    "        exclude = False\n",
    "        for word in sent:\n",
    "            if word == '?' or word == ';' or word == '...':\n",
    "                exclude = True\n",
    "                break\n",
    "        if exclude:\n",
    "            continue\n",
    "\n",
    "        tokens = tag_tokens(sent)\n",
    "        buff.extend(tokens)          \n",
    "\n",
    "        if prev_offset > 0:                \n",
    "            if buff[prev_offset - 1].text == '.':\n",
    "                del buff[prev_offset - 1]\n",
    "                buff[prev_offset - 2] = buff[prev_offset - 2]._replace(point_after = True)                \n",
    "\n",
    "                if not buff[prev_offset - 1].text.isupper():                \n",
    "                    lwr = random.random()                \n",
    "                    if lwr < 0.93: #псуємо сase\n",
    "                        buff[prev_offset - 1] = buff[prev_offset -1].\\\n",
    "                        _replace(text = buff[prev_offset - 1].text.lower())                                            \n",
    "\n",
    "        prev_offset = len(buff)            \n",
    "\n",
    "        if run_on_num == 0:            \n",
    "            corpus.append(buff)\n",
    "            run_on_num = rand_run_on_num()\n",
    "            prev_offset = 0\n",
    "            buff = []                               \n",
    "\n",
    "        run_on_num -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18701"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемішуємо корпус та розділяємо на тренувальні і тестові датасети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    random.shuffle(corpus)\n",
    "\n",
    "train_index = int(0.7 * len(corpus))\n",
    "\n",
    "train_data = corpus[: train_index]\n",
    "test_data = corpus[train_index: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Збираємо біграми та триграми на тестових данних, які також включають крапки на кінці речення. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = defaultdict(float)\n",
    "trigrams = defaultdict(float)\n",
    "\n",
    "all_tokens = [token for sent in train_data for token in map(lambda x: x.text.lower(),sent)]\n",
    "\n",
    "for g in nltk.ngrams(all_tokens, 2):        \n",
    "    bigrams[g] += 1.0\n",
    "                \n",
    "for g in nltk.ngrams(all_tokens, 3):    \n",
    "    trigrams[g] += 1.0\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "def update_freq(ngrams):\n",
    "    n = sum(ngrams.values())\n",
    "    for k, v in ngrams.items():\n",
    "        ngrams[k] = v/n \n",
    "\n",
    "update_freq(bigrams)\n",
    "update_freq(trigrams)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', 'the'), 0.00598490423501983),\n",
       " (('of', 'the'), 0.0055116691522362525),\n",
       " ((\"'\", 's'), 0.00521861072941156),\n",
       " (('in', 'the'), 0.004758400465420191),\n",
       " (('said', '.'), 0.003377769673446085),\n",
       " ((',', 'the'), 0.003186738997827026),\n",
       " (('said', 'the'), 0.0029913667159438976),\n",
       " ((',', '000'), 0.0029544630626993067),\n",
       " (('mln', 'dlrs'), 0.0028524353154936732),\n",
       " (('u', '.'), 0.0026852834743269966),\n",
       " (('the', 'company'), 0.002631013396026128),\n",
       " (('.', '\"'), 0.002572401711461189),\n",
       " (('.', 's'), 0.002500765208104042),\n",
       " (('s', '.'), 0.002405249870294513),\n",
       " ((',', '\"'), 0.002403079067162478),\n",
       " (('for', 'the'), 0.0021121914474698206),\n",
       " (('to', 'the'), 0.001901623543662449),\n",
       " ((',', 'and'), 0.0018191330246451279),\n",
       " (('said', 'it'), 0.0017995957964568152),\n",
       " (('1', '.'), 0.0017995957964568152)]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bigrams.items(), key=lambda kv: kv[1], reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('u', '.', 's'), 0.0024942582132514795),\n",
       " (('.', 's', '.'), 0.0023640097425856057),\n",
       " (('the', 'company', 'said'), 0.0011678946203039998),\n",
       " (('the', 'u', '.'), 0.0011244784634153755),\n",
       " (('.', 'the', 'company'), 0.0008704939456169219),\n",
       " (('said', '.', 'the'), 0.0008031989024395538),\n",
       " ((',', '000', 'dlrs'), 0.0007098541651290112),\n",
       " ((',', '000', 'tonnes'), 0.0006664380082403866),\n",
       " (('he', 'said', '.'), 0.0006382175062627807),\n",
       " (('.', 'it', 'said'), 0.0006186802356628996),\n",
       " ((',', 'it', 'said'), 0.0005839473101519999),\n",
       " ((',', '\"', 'he'), 0.0005665808473965502),\n",
       " ((',', 'he', 'said'), 0.0004927733806858885),\n",
       " (('mln', 'dlrs', 'in'), 0.0004819193414637323),\n",
       " (('mln', 'dlrs', ','), 0.0004797485336193011),\n",
       " (('.', '\"', 'the'), 0.0004558696473305576),\n",
       " ((',', 'the', 'company'), 0.0004493572237972639),\n",
       " (('.', '5', 'pct'), 0.00042547833750852045),\n",
       " (('\"', 'he', 'said'), 0.00041896591397522676),\n",
       " (('it', 'said', '.'), 0.0004059410669086394)]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(trigrams.items(), key=lambda kv: kv[1], reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Намагаємось побудувати простий безлайн на правилах, та перевіямо якість на тестових датасетах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def baseline(data):\n",
    "    result_data = []\n",
    "    for sent in data:\n",
    "        result_sent = []\n",
    "        last_point = 0\n",
    "        for i, word in enumerate(sent):            \n",
    "            if (i - last_point) > 3 and i < (len(sent) - 1):\n",
    "                pbigram = (word[0].lower(), '.')                \n",
    "                bigram = (word[0].lower(), sent[i+1][0].lower())                                    \n",
    "                if (sent[i + 1][0][0:1].isupper()):\n",
    "                    result_sent.append([word[0], True])\n",
    "                elif math.log(bigrams[pbigram] + 0.000000000001) > math.log(bigrams[bigram] + 0.001):                        \n",
    "                    result_sent.append([word[0], True])\n",
    "                    last_point = i\n",
    "                else:                       \n",
    "                    result_sent.append([word[0], False])                                    \n",
    "            else:                \n",
    "                result_sent.append([word[0], False])\n",
    "            \n",
    "        result_data.append(result_sent)   \n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = baseline(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# виділяє вектор лейблів з розміченних данних\n",
    "def labels_vec(data):\n",
    "    return [word[1] for sent in data for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.91      0.95    196267\n",
      "        True       0.02      0.18      0.03      1655\n",
      "\n",
      "   micro avg       0.91      0.91      0.91    197922\n",
      "   macro avg       0.51      0.55      0.49    197922\n",
      "weighted avg       0.98      0.91      0.94    197922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(labels_vec(test_data), labels_vec(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.96      0.97      4542\n",
      "        True       0.26      0.45      0.33       155\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      4697\n",
      "   macro avg       0.62      0.70      0.65      4697\n",
      "weighted avg       0.96      0.94      0.95      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(run_on_js), labels_vec(baseline(run_on_js))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Далі будуємо і тренуємо модель базовану на логістичній регресії, порівнюємо 2 варіанта за n-грамами та без них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def bigram_log_freq(word1, word2):\n",
    "    return math.log((0.0 if (word1 is None) or (word2 is None)\\\n",
    "                    else bigrams[(word1.lower(), word2.lower())]) + 0.000001)\n",
    "\n",
    "def trigram_log_freq(word1, word2, word3):\n",
    "    return math.log((0.0 if (word1 is None) or (word2 is None) or (word3 is None)\\\n",
    "                    else trigrams[(word1, word2, word3)]) + 0.000001)\n",
    "\n",
    "def get_prop(token, name, default):\n",
    "    return default if token is None else getattr(token, name)\n",
    "\n",
    "def extract_sent_features(sent, use_ngrams):\n",
    "    features = []\n",
    "    next1_token = sent[1] if len(sent) > 1 else None\n",
    "    next2_token = sent[2] if len(sent) > 2 else None\n",
    "    prev1_token = None\n",
    "    prev2_token = None    \n",
    "    for i, token in enumerate(sent):\n",
    "        fdic = {}                \n",
    "        fdic['next_case1'] = get_prop(next1_token, 'text', '')[:1].isupper()        \n",
    "        fdic['case'] = token.text[:1].isupper()\n",
    "        \n",
    "        fdic['tag'] = token.tag\n",
    "        fdic['tag_prev1'] = get_prop(prev1_token, 'tag', 'NONE_TAG')\n",
    "        fdic['tag_next1'] = get_prop(next1_token, 'tag', 'NONE_TAG')\n",
    "        fdic['tag_prev2'] = get_prop(prev2_token, 'tag', 'NONE_TAG')\n",
    "        fdic['tag_next2'] = get_prop(next2_token, 'tag', 'NONE_TAG')\n",
    "        \n",
    "        fdic['index1'] = len(sent)/(i + 1)\n",
    "        fdic['index2'] = len(sent)/(len(sent) - i +  1)\n",
    "        \n",
    "        fdic['word'] = token.text\n",
    "        fdic['prev_word1'] = get_prop(prev1_token, 'text', '')\n",
    "        fdic['next_word1'] = get_prop(next1_token, 'text', '')\n",
    "        fdic['prev_word2'] = get_prop(prev2_token, 'text', '')\n",
    "        fdic['next_word2'] = get_prop(next2_token, 'text', '')\n",
    "                \n",
    "        if use_ngrams:\n",
    "            fdic['bigram'] = bigram_log_freq(token.text, get_prop(next1_token, 'text', None))\n",
    "            fdic['pbigram'] = bigram_log_freq(token.text, '.')        \n",
    "            fdic['pbigram2'] = bigram_log_freq('.', get_prop(next1_token, 'text', None)) \n",
    "            fdic['trigram'] = trigram_log_freq(get_prop(prev1_token, 'text', None), token.text, get_prop(next1_token, 'text', None))\n",
    "            fdic['ptrigram'] = trigram_log_freq(token.text, '.', get_prop(next1_token, 'text', None))         \n",
    "            fdic['ptrigram2'] = trigram_log_freq('.', get_prop(next1_token, 'text', None), get_prop(next2_token, 'text', None)) \n",
    "            fdic['ptrigram3'] = trigram_log_freq(get_prop(prev2_token, 'text', None), get_prop(prev1_token, 'text', None), '.')        \n",
    "        \n",
    "        features.append(fdic)\n",
    "        \n",
    "        prev2_token = prev1_token\n",
    "        prev1_token = token\n",
    "        \n",
    "        next1_token = next2_token\n",
    "        next2_token = sent[i+3] if (i+3) < len(sent) else None\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features(sents, use_ngrams):\n",
    "    features = []\n",
    "    for sent in sents:\n",
    "        features.extend(extract_sent_features(sent, use_ngrams))\n",
    "    return features     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "class RunOnModel:\n",
    "    def __init__(self, use_ngrams):\n",
    "        self.use_ngrams = use_ngrams\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        self.logreg = LogisticRegression(random_state=26, solver='liblinear', multi_class='ovr', max_iter=3000)\n",
    "        \n",
    "    def train(self, data):\n",
    "        features = extract_features(data, self.use_ngrams)\n",
    "        self.vectorizer.fit(features)\n",
    "        feature_vecs = self.vectorizer.transform(features)\n",
    "        self.logreg.fit(feature_vecs, labels_vec(data))\n",
    "        \n",
    "    def predict(self, data):\n",
    "        vec = self.vectorizer.transform(extract_features(data, self.use_ngrams))\n",
    "        return self.logreg.predict(vec)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_mod = RunOnModel(use_ngrams = False)\n",
    "simple_mod.train(train_data)\n",
    "\n",
    "ngram_mod = RunOnModel(use_ngrams = True)\n",
    "ngram_mod.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00    196267\n",
      "        True       0.84      0.45      0.59      1655\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    197922\n",
      "   macro avg       0.92      0.73      0.79    197922\n",
      "weighted avg       0.99      0.99      0.99    197922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(test_data), simple_mod.predict(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00    196267\n",
      "        True       0.68      0.66      0.67      1655\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    197922\n",
      "   macro avg       0.84      0.83      0.83    197922\n",
      "weighted avg       0.99      0.99      0.99    197922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(test_data), ngram_mod.predict(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далі перевіряемо натреновану модель на наданих тестових данних, але перед цим тегаємо данні та виправяемо лейбли, оскліьки токенізація може відрізнятись."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tokens(data):\n",
    "    tagged_sents = []\n",
    "    \n",
    "    for sent in data:\n",
    "        tagged_sent = [] \n",
    "        tokens = tag_tokens(list(map(lambda w: w[0], sent)))\n",
    "        compound = None\n",
    "        merged = 0\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while i < len(tokens) or j < len(sent): \n",
    "            orig_token = sent[i] if i < len(sent) else sent[len(sent) - 1]\n",
    "            token = tokens[j] if j < len(tokens) else tokens[len(tokens) - 1]           \n",
    "            if orig_token[0] == token.text:                \n",
    "                tagged_sent.append(token._replace(point_after = orig_token[1]))\n",
    "                j += 1\n",
    "                i += 1\n",
    "            elif orig_token[0] in token.text:                \n",
    "                if not compound:\n",
    "                    compound = token\n",
    "                else:\n",
    "                    compound = compound._replace(point_after = orig_token[1])\n",
    "                merged += len(orig_token[0])    \n",
    "                if merged == len(compound.text):                     \n",
    "                    j += 1\n",
    "                    tagged_sent.append(compound)\n",
    "                    compound = None                \n",
    "                    merged = 0\n",
    "                i += 1    \n",
    "            elif token.text in orig_token[0]:                \n",
    "                if not compound:\n",
    "                    compound = orig_token[0]\n",
    "                    \n",
    "                merged += len(token.text)\n",
    "                if merged == len(compound): \n",
    "                    i += 1\n",
    "                    tagged_sent.append(token._replace(point_after = orig_token[1]))                    \n",
    "                    compound = None\n",
    "                    merged = 0\n",
    "                else:\n",
    "                    tagged_sent.append(token)\n",
    "                j += 1\n",
    "            else:                              \n",
    "                assert False                                    \n",
    "        \n",
    "        tagged_sents.append(tagged_sent)\n",
    "    \n",
    "    return tagged_sents\n",
    "\n",
    "run_on_js_tokenized = convert_to_tokens(run_on_js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "print(sum([w[1] for sent in run_on_js for w in sent]))\n",
    "print(sum([t.point_after for sent in run_on_js_tokenized for t in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.99      4618\n",
      "        True       0.78      0.14      0.23       155\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      4773\n",
      "   macro avg       0.87      0.57      0.61      4773\n",
      "weighted avg       0.97      0.97      0.96      4773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(run_on_js_tokenized), simple_mod.predict(run_on_js_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      4618\n",
      "        True       0.73      0.38      0.50       155\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      4773\n",
      "   macro avg       0.85      0.69      0.74      4773\n",
      "weighted avg       0.97      0.98      0.97      4773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(run_on_js_tokenized), ngram_mod.predict(run_on_js_tokenized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Висновки\n",
    "\n",
    "Якщо порівнювати моделі з урахуванням n-gram та без них, то ми бачимо, що використання n-gram підвищує recall але з іншої сторони впливає на precision з негативної сторони. На мій погляд precision для цієї задачи більш важливий. \n",
    "\n",
    "Можу зробити припущення, що якщо використати більший тренувальний датасет та взяти більший датасет, то точність моделі можна підвищити.\n",
    "\n",
    "Я також намагався використати дерева складників, наприклад врахувувати найближчого спільного батька, але всі парсери (https://pypi.org/project/bllipparser/ , https://github.com/nikitakit/self-attentive-parser) які я спробував працювали занадто довго. Але я думаю перспектива в цьому напряму є."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
