{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Task"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. (а (тип + овий)\n",
    "2. (море +(плав + ання) )\n",
    "3. (о ( (по + дат(и) ) + кувати ) )\n",
    "4. ( (перев + (тіл) ) + ити)ся)\n",
    "5. (с + ((хили) + вшись) )\n",
    "6. (під + (сніж) +ник)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = stem.snowball.SnowballStemmer('english')\n",
    "tokens = ['truth', 'truthful', 'truthfulness', 'countertruth', 'untruthful', 'truthology']\n",
    "# Perform stemming on the tokenized words \n",
    "tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "tokens_stemmed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cтемер явно гарно виділяє та забирає суфікси (-ful творення прикметника, -ness творення іменника).\n",
    "З префіксами на даному прикладі не працює взагалі (-un та -counter)\n",
    "-ology творення іменника суфікс не був виділений, бо спочатку було відрізано суфікс (-y  творення іменника)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flaw', 'flaw', 'flaw', 'flawless', 'flawless', 'flawless']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['flaw', 'flaws', 'flawed', 'flawless', 'flawlessness', 'flawlessly']\n",
    "tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "tokens_stemmed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Суфікс множини -s, -ed, -ness, -ly аналогічно були забрані. \n",
    "\n",
    "Суфікс -less та префікси -un та -counter  були залишений в усіх випадках. \n",
    "\n",
    "Думаю, оскільки ці афікси заперечні, вони надають слову інше значення, фактично протилежне (наприклад, flaw - недолік, flawless-бездоганний),\n",
    "то має сенс залишити саме цю форму. Наприклад, для аналізу сентиментів воно би зіграло роль. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лес', 'лесн', 'лесник', 'леснич', 'лесничеств', 'пролес']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = stem.snowball.SnowballStemmer('Russian')\n",
    "tokens = ['лес', 'лесной', 'лесник', 'лесничий', 'лесничество', 'пролесье']\n",
    "tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "tokens_stemmed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "У більшості випадків відрізались тільки суфікси з крайної правої сторони.\n",
    "Префік проігноровано :)\n",
    "З сайту http://snowball.tartarus.org\n",
    "One can make some distinction between root and stem. Lovins (1968) sees the root as the stem minus any prefixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['окн', 'окошк', 'подоконник', 'окон', 'окнищ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['окно', 'окошко', 'подоконник', 'оконный', 'окнище']\n",
    "tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "окн - корінь\n",
    "забрано закінчення -о та -е\n",
    "Потрібно розділяти поняття stem(основа слова) та root(корінь)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can {but} hope that everything will be fine.:  conjunction, сс(can, but)  \n",
    "It's sad {but} true.: conjunction, сс(sad, but)\n",
    "Jack brings nothing {but} trouble.: preposition, case(trouble, case) \n",
    "{As} we were talking, I realised how to solve the issue.: conjunction, сс(talking, as)\n",
    "This hot dog isn't {as} big as usual.: adverb, advmod(big, as)\n",
    "This hot dog isn't as big {as} usual.: preposition, case(usual, as)\n",
    "This hot dog isn't as big {as} I expected.: conjunction, сс(talking, as)\n",
    "I work {as} a teacher.: preposition, case(teacher, as)\n",
    "Let's do it this {way}!: noun, xcomp(do, way)\n",
    "This is {way} too much!: noun, advmod(much, way)\n",
    "The prices are going {down}. particle, compound:prt(going, down)\n",
    "Someone pushed him and he fell {down} the stairs.: adverb,  compound:prt(fell, down)\n",
    "I’ve been feeling rather {down} lately.: adverb, advmod(feeling, down)\n",
    "It's not easy to {down} a cup of coffee in one gulp.: verb, xcomp(easy, down)\n",
    "Bring a {down} jacket and a pair of gloves, and you'll be fine.: adjective, amod( jacket, down) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Рада міністрів Європейського союзу затвердила угоду про спрощений порядок видачі {віз} для України.:\n",
    "    іменник, nmod( видачі,віз)\n",
    "Батько Себастьяна {віз} на санях їх театральний гурт до Львова.:\n",
    "    дієслово, nsubj(віз, Батько)\n",
    "А ще дивний елемент інтер’єру – {віз} із продукцією одного з херсонських виробників.:\n",
    "    іменнник, csubj(дивний, віз)\n",
    "У цю мить {повз} Євгена пролетів останній вагон товарняка.:\n",
    "    прийменник, case(Євгена, повз)\n",
    "Кліпнув очима і побачив малого песика, який саме пробігав {повз} у бік села.:\n",
    "    прислівник, advmod(пробігав, повз)\n",
    "Степанко перестав кричати, тільки ламкий стогін {повз} йому із грудей.:\n",
    "   дієслово, advcl(кричати, повз)\n",
    "Часу не {гай} – декларацію подай!:\n",
    "    дієслово, root(root, гай)\n",
    "І коляд заспівали, і {гай} врятували.:\n",
    "    іменник, dobj(врятували, гай)\n",
    "{Гай}, чи ви забулися, братове?:\n",
    "    вигук, discourse(забулися,Гай)\n",
    "Ось присіла на {край} ліжка.:\n",
    "    іменник, nmod(присіла, край)\n",
    "Поставив ту кузню не {край} дороги, як було заведено, а на Красній горі, біля Прадуба.:\n",
    "    прислівник, advmod(Поставив, край)\n",
    "Розповідаючи про передній {край} лінґвістики, фон Лібіх, як завжди, мислив широко і глобально.:\n",
    "    іменник, nmod(розповідаючи, край)\n",
    "Не {край} мені серце.:\n",
    "    дієслово, root(root, край)\n",
    "І {щойно} тоді додаємо до борщу смажену цибулю.:\n",
    "    прислівник, advmod(додаємо, щойно)  \n",
    "Бо {щойно} я задрімав, віддавши тіло подушці з периною, як мене розбудив поштовх у бік.:\n",
    "    сполучник, cc(задрімав, щойно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) Куля - стереометричне тіло, утворене обертанням кола навколо свого діаметра.\n",
    "синоніми: сфера\n",
    "антоніми: - \n",
    "мероніми: діаметр, радіус, сектор, сегмент.\n",
    "голоніми - \n",
    "гіпоніми: - \n",
    "гіпероніми: округла форма, форма, абстрактиний об'єкт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a projectile that is fired from a gun\n",
      "1 a high-speed passenger train\n",
      "2 (baseball) a pitch thrown with maximum velocity\n",
      "Hypernyms: [Synset('projectile.n.01')]\n",
      "Hyponyms: [Synset('dumdum.n.01'), Synset('full_metal_jacket.n.01'), Synset('rifle_ball.n.01'), Synset('rubber_bullet.n.01')]\n",
      "meronyms: []\n",
      "holonyms: []\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "word='bullet'\n",
    "syn_list = wn.synsets(word)\n",
    "for num, i in enumerate(syn_list):\n",
    "    print(num, i.definition())\n",
    "choose_syn = syn_list[0]\n",
    "print('Hypernyms:', choose_syn.hypernyms())\n",
    "print('Hyponyms:', choose_syn.hyponyms() )\n",
    "print('meronyms:', choose_syn.member_meronyms())\n",
    "print('holonyms:', choose_syn.member_holonyms() )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2) Куля - головна частина набою, що вилітає із вогнепальної зброї.\n",
    "синоніми: снаряд, набій, патрон\n",
    "антоніми: -  \n",
    "мероніми: порох, ка́псуль, гільза\n",
    "голоніми - зброя, боєприпас, пістолет\n",
    "гіпоніми: - гумова куля \n",
    "гіпероніми: зброя, інструмент, одиниця, об'єкт, фізична річ "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3) Куля - земна куля, планета Земля\n",
    "синоніми: Земля, планета, глобус, суходіл\n",
    "антоніми: -  \n",
    "мероніми: океани, материки, атмосфера, біосефра, гідросфера\n",
    "голоніми - сонячна система, геліосфера, галактика, світ\n",
    "гіпоніми: - \n",
    "гіпероніми:  одиниця, об'єкт, фізична річ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Форматування"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import NOUN, PROPN, PRON, VERB, ADJ, ADV, ADP\n",
    "# tag- IN, pos - ADP, conjunction, subordinating or preposition\n",
    "nlp = spacy.load('en')\n",
    "# іменники, займенники, дієслова, прикметники, прислівники та підрядні сполучники\n",
    "target_pos_tags = ('NOUN', 'PROPN', 'PRON', 'VERB', 'ADJ', 'ADV', 'ADP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines_fron_txt(file_name):\n",
    "    \n",
    "    f = open(file_name)\n",
    "    f_lines = f.readlines()\n",
    "    file_in.close()\n",
    "    return f_lines \n",
    "\n",
    "def capitalize_1(word):\n",
    "    \n",
    "    if word == word.upper():\n",
    "        return word\n",
    "    else:\n",
    "        return word.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_headlines(file_in, file_out):\n",
    "    \n",
    "    f_lines = read_lines_fron_txt(file_in)\n",
    "    # append data to file\n",
    "    f_out = open( file_out, \"a+\")\n",
    "    for line in f_lines:\n",
    "#         print(line)\n",
    "        doc = nlp(line)\n",
    "        new_line = capitalize_1(doc[0].text)\n",
    "        for token in doc[1:-1]:\n",
    "            curr_token = token.text\n",
    "            if(token.pos_ in target_pos_tags and token.dep_ != 'prep'):\n",
    "                curr_token = (capitalize_1(token.text))\n",
    "            new_line = new_line +  \" \"*(token.idx - len(new_line)) + curr_token\n",
    "        new_line = new_line + capitalize_1(doc[-1].text)\n",
    "#         print(new_line)\n",
    "        f_out.write(new_line)\n",
    "    f_out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_headlines(file_in ='examiner-headlines.txt', file_out=\"examiner-headlines_out.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Вірусні новини"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чи є в заголовку іменовані стуності?\n",
    "Чи є в заголовку прикметники та прислівники вищого і найвищого ступенів порівняння?\n",
    "Чи є заголовок позитивно чи негативно забарвлений?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible entities:\n",
    "PERSON\tPeople, including fictional.\n",
    "NORP\tNationalities or religious or political groups.\n",
    "FAC\tBuildings, airports, highways, bridges, etc.\n",
    "ORG\tCompanies, agencies, institutions, etc.\n",
    "GPE\tCountries, cities, states.\n",
    "LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "WORK_OF_ART\tTitles of books, songs, etc.\n",
    "LAW\tNamed documents made into laws.\n",
    "LANGUAGE\tAny named language.\n",
    "DATE\tAbsolute or relative dates or periods.\n",
    "TIME\tTimes smaller than a day.\n",
    "PERCENT\tPercentage, including \"%\".\n",
    "MONEY\tMonetary values, including unit.\n",
    "QUANTITY\tMeasurements, as of weight or distance.\n",
    "ORDINAL\t\"first\", \"second\", etc.\n",
    "CARDINAL\tNumerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_1_entities(doc, target_ent_labels=['PERSON', 'EVENT', 'ORG']):\n",
    "    \"\"\"\n",
    "    Return 0 = no entities, 1 = one or more entities\n",
    "    \"\"\"\n",
    "    \n",
    "    ent_labels = [token.ent_type_ for token in doc]\n",
    "    has_name_entity = int(bool(set(target_ent_labels) & set(ent_labels)))\n",
    "    return has_name_entity\n",
    "\n",
    "def get_feature_2_com_sup(doc, target_tags=['JJR', 'JJS', 'RBR', 'RBS']):\n",
    "    \"\"\"\n",
    "    Return 0 = no entries, 1 = at least one adjective or adverb -  comparative/ superlative\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_tags = [token.tag_ for token in doc]\n",
    "    has_comparative_superlative = int(bool(set(target_tags) & set(pos_tags)))\n",
    "    return has_comparative_superlative\n",
    "\n",
    "\n",
    "def get_feature_3_swn_emotion(raw_sentence):\n",
    "    \"\"\"\n",
    "    Return 0 = no emotion, 1 = emotion\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentiment = 0.0\n",
    "\n",
    "    tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            continue\n",
    "            \n",
    "        # Try to take 5 synsets \n",
    "        # Take the first synset\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "        sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "\n",
    "    # print(sentiment)\n",
    " \n",
    "    # Has an emotion\n",
    "    if sentiment > 0.3 or sentiment < -0.3:\n",
    "        return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_feature_df(file_name='examiner-headlines.txt'):\n",
    "    f_lines = read_lines_fron_txt(file_name)\n",
    "    stat= []\n",
    "    for line in f_lines: \n",
    "        doc = nlp(line)   \n",
    "        stat.append([get_feature_1_entities(doc=doc),\\\n",
    "                     get_feature_2_com_sup(doc=doc),\\\n",
    "                     get_feature_3_swn_emotion(raw_sentence=line)])\n",
    "    df = pd.DataFrame(stat, columns=['has_entity', 'has_com_sup', 'has_emotion'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = get_feature_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has at least one entity of [PERSON, EVENT, ORG]: 59.56%\n"
     ]
    }
   ],
   "source": [
    "print('Has at least one entity of [PERSON, EVENT, ORG]: {}%'\\\n",
    "      .format(100*df['has_entity'].sum()/df['has_entity'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has at least one adj/adv + comparative/ superlative: 4.48%\n"
     ]
    }
   ],
   "source": [
    "print('Has at least one adj/adv + comparative/ superlative: {}%'\\\n",
    "      .format(100*df['has_com_sup'].sum()/df['has_com_sup'].count()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ознака \"має емоцію\" була обрана настпуним чином:\n",
    "    визначаємо для слова з рядку сентимент за першим синсетом як позитивний score \"-\" негативний score\n",
    "    score емоційного забарвлення рядку визначаємо як сума по словах з рядку\n",
    "    якщо score рядку > 0.3 або <-0.3, то рядок має емоцію"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has an emotion: 28.44%\n"
     ]
    }
   ],
   "source": [
    "print('Has an emotion: {}%'\\\n",
    "      .format(100*df['has_emotion'].sum()/df['has_emotion'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Колокації"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSynonyms(lemma, pos=wn.VERB):\n",
    "    synsets = wn.synsets(lemma, pos=pos)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    else:\n",
    "        synonyms =[]\n",
    "        for syn in synsets:\n",
    "            synonyms = synonyms + [l.name() for l in syn.lemmas()]\n",
    "        return(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say',\n",
       " 'differentiate',\n",
       " 'arrogate',\n",
       " 'transmit',\n",
       " 'claim',\n",
       " 'allege',\n",
       " 'secern',\n",
       " 'secernate',\n",
       " 'severalize',\n",
       " 'assure',\n",
       " 'suppose',\n",
       " 'distinguish',\n",
       " 'read',\n",
       " 'mouth',\n",
       " 'communicate',\n",
       " 'evidence',\n",
       " 'pass',\n",
       " 'state',\n",
       " 'order',\n",
       " 'enjoin',\n",
       " 'enunciate',\n",
       " 'verbalise',\n",
       " 'aver',\n",
       " 'recite',\n",
       " 'utter',\n",
       " 'tell',\n",
       " 'enounce',\n",
       " 'articulate',\n",
       " 'severalise',\n",
       " 'separate',\n",
       " 'speak',\n",
       " 'intercommunicate',\n",
       " 'narrate',\n",
       " 'recount',\n",
       " 'exact',\n",
       " 'pronounce',\n",
       " 'verbalize',\n",
       " 'convey',\n",
       " 'talk',\n",
       " 'take',\n",
       " 'commune',\n",
       " 'address']"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = [\"say\", \"tell\", \"speak\", \"claim\", \"communicate\"]\n",
    "synonyms = []\n",
    "for lemma in lemmas:\n",
    "    synonyms = synonyms + getSynonyms(lemma, pos=wn.VERB)\n",
    "synonyms = [item for item in synonyms if '_' not in item]\n",
    "synonyms = list(set(synonyms))\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getCollocations(file_name='blog2008.txt'):\n",
    "    f_lines = read_lines_fron_txt(file_name)\n",
    "\n",
    "    verb_adv_list = []\n",
    "    for line in tqdm(f_lines):\n",
    "        doc = nlp(line)\n",
    "        for token in doc:\n",
    "            if(token.lemma_ in synonyms and token.pos_ == 'VERB'):\n",
    "                for child in token.children:\n",
    "                    if child.pos_ == 'ADV' and child.dep_ == 'advmod' and child.text[-2:] == 'ly':\n",
    "    #                     print(token.lemma_, child.text, child.pos_, child.dep_)\n",
    "                        verb_adv_list.append([token.lemma_, child.text])\n",
    "    #                     print(line)\n",
    "    return verb_adv_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 303994/303994 [1:04:51<00:00, 78.12it/s]\n"
     ]
    }
   ],
   "source": [
    "verb_adv_list = getCollocations(file_name='blog2008.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "save_obj(verb_adv_list, 'my_collocations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cols =  pd.DataFrame(verb_adv_list, columns=['verb', 'adverb'])\n",
    "df_cols = df_cols.assign(count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_agg = df_cols.groupby(['verb', 'adverb']).agg({'count':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    " g = df_agg['count'].groupby(level=0, group_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " res = g.apply(lambda x: x.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adverb\n",
       "recently      73\n",
       "actually      73\n",
       "repeatedly    55\n",
       "simply        45\n",
       "explicitly    39\n",
       "publicly      36\n",
       "basically     30\n",
       "really        29\n",
       "only          25\n",
       "clearly       22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['say']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adverb\n",
       "recently      25\n",
       "reportedly    14\n",
       "privately     10\n",
       "basically      9\n",
       "only           9\n",
       "repeatedly     8\n",
       "simply         8\n",
       "really         7\n",
       "probably       7\n",
       "actually       7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['tell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adverb\n",
       "directly        34\n",
       "publicly        15\n",
       "fiercely        12\n",
       "only            10\n",
       "recently         8\n",
       "loudly           7\n",
       "clearly          7\n",
       "openly           7\n",
       "actually         6\n",
       "figuratively     5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['speak'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adverb\n",
       "falsely       50\n",
       "Falsely       17\n",
       "previously     8\n",
       "repeatedly     8\n",
       "actually       3\n",
       "laughably      3\n",
       "publicly       3\n",
       "credibly       3\n",
       "initially      3\n",
       "recently       3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['claim'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adverb\n",
       "effectively    3\n",
       "directly       3\n",
       "regularly      1\n",
       "quickly        1\n",
       "profoundly     1\n",
       "privately      1\n",
       "poorly         1\n",
       "loudly         1\n",
       "hopefully      1\n",
       "freely         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"communicate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.reset_index().to_csv('collocations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
