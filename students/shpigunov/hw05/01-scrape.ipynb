{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Набір слів\n",
    "\n",
    "## I. Сентимент-аналіз\n",
    "\n",
    "Цього тижня ви побудуєте класифікатор для аналізу тональності тексту (позитивна, негативна чи нейтральна). Даними будуть споживацькі відгуки з сайту https://rozetka.com.ua/, написані українською мовою.\n",
    "\n",
    "### 1. Домен\n",
    "\n",
    "- виберіть категорію товарів на https://rozetka.com.ua/\n",
    "- зіскрейпіть відгуки користувачів разом із кількістю зірочок, яку поставив користувач\n",
    "- відфільтруйте відгуки українською мовою, використавши будь-яку бібліотеку для визначення мови\n",
    "- зберіть кілька тисяч відгуків та поділіть дані на тренувальні та тестувальні\n",
    "\n",
    "### 2. Класифікатор\n",
    "\n",
    "- виберіть будь-який bag-of-words класифікатор (Naive Bayes, Averaged Perceptron, Logistic Regression, SVM, etc.) та імплементуйте першу версію сентимент-аналізу; поміряйте якість на тестовій вибірці\n",
    "- використайте [тональний словник](https://github.com/lang-uk/tone-dict-uk) для покращення якості класифікатора; поміряйте якість на тестовій вибірці\n",
    "- спробуйте покращити якість роботи класифікатора іншими способами (фільтрування стоп-слів, використання лем слів, опрацювання заперечень, ваші варіанти)\n",
    "\n",
    "Запишіть ваші спостереження та результати в окремий файл.\n",
    "\n",
    "Крайній термін: 6.04.2019 (100% за завдання)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 100 of 100...Done.\n",
      "Item 3115 / 3116, page 1 / 1, reviews: 0"
     ]
    }
   ],
   "source": [
    "\"\"\"rozetka.com.ua scraper\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import langdetect\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Local module\n",
    "from persistent_index import PersistentIndex as pi\n",
    "from persistent_index import PersistentKeyValueStorage as kvs\n",
    "\n",
    "\n",
    "STORAGE_FILE = 'notebooks'\n",
    "\n",
    "def run_request(url):\n",
    "    \"\"\"Fetch & soupify url. Use sparingly\"\"\"\n",
    "    \n",
    "    # This will mask us as a legit user and help avoid 424\n",
    "    headers = {\n",
    "        'User-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15',\n",
    "        'Referer': 'https://rozetka.com.ua/',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Encoding': 'br, gzip, deflate',\n",
    "        'Host': 'rozetka.com.ua',\n",
    "        'Connection': 'keep-alive'\n",
    "    }\n",
    "        \n",
    "    # session = trident.init()\n",
    "    session = requests.session()\n",
    "    # session.headers.update(headers)\n",
    "    \n",
    "    # Run request\n",
    "    r = session.get(url, headers=headers)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    \n",
    "        \n",
    "    # Return a BS4 object\n",
    "    return soup\n",
    "\n",
    "def get_number_of_pages(url):\n",
    "    \n",
    "    # Get parsed html\n",
    "    soup = run_request(url)\n",
    "    \n",
    "    # Find number of pages\n",
    "    pages = soup.find_all('span', class_ = 'paginator-catalog-l-i-active')\n",
    "    if pages:\n",
    "        return int(pages[-1].text)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def scrape_links_on_page(root_url, page=1):\n",
    "    \n",
    "    # Initialize PersistentIndex object [instead of all_links]\n",
    "    \n",
    "    global STORAGE_FILE\n",
    "    index = pi(db_filename=f'./persistent_index/{STORAGE_FILE}.sqlite', \n",
    "               table_name='product_links')\n",
    "    \n",
    "    # Get parsed html of root url\n",
    "    soup = run_request(f\"{root_url}page={page}\")\n",
    "    \n",
    "    # Parse link and add it to the PersistentIndex\n",
    "    for tag in soup.find_all('div', class_='g-i-tile-i-title'):\n",
    "        link = tag.a.get('href')\n",
    "        index.add_unvisited(link)\n",
    "    index.close()\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def scrape_links_in_section(root_url):\n",
    "    \n",
    "    num_pages = get_number_of_pages(root_url)\n",
    "    \n",
    "    for i in range(1, num_pages+1):\n",
    "        scrape_links_on_page(root_url, i)\n",
    "        print(f'\\rScraping {i} of {num_pages}...', end='')\n",
    "    print(\"Done.\")\n",
    "    \n",
    "\n",
    "def find_reviews(url, page = 1):\n",
    "    \"Find all reviews on a page\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # Init key-value storage\n",
    "    storage = kvs(db_filename=f'./persistent_index/{STORAGE_FILE}.sqlite', \n",
    "                  table_name='kvs')      \n",
    "    \n",
    "    target_url = f\"{url}/page={page}\"\n",
    "    soup = run_request(target_url)\n",
    "    reviews = soup.find_all('div', {'itemprop': 'review'})\n",
    "    \n",
    "    for review in reviews:\n",
    "        score = review.find('span', class_='g-rating-stars-i')\n",
    "        if score:\n",
    "            score = int(score.get('content'))\n",
    "\n",
    "        comment = review.find('div', class_='pp-review-text')\n",
    "        \n",
    "        if comment:\n",
    "            comment = comment.text\n",
    "\n",
    "        if score and len(comment) > 3:\n",
    "            try:\n",
    "                lang = langdetect.detect(comment)\n",
    "                if lang == 'uk':\n",
    "                    result.append({'Text': comment, 'Score': score})\n",
    "                    storage.insert(comment, score)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Store fetched content before return\n",
    "    storage.save_state()\n",
    "    \n",
    "    return result\n",
    "                \n",
    "\n",
    "def main0(root_url):\n",
    "    \n",
    "    # Find number of pages in section\n",
    "    pages = get_number_of_pages(root_url)\n",
    "    \n",
    "    total_reviews = []\n",
    "    \n",
    "    # Scrape product links\n",
    "    scrape_links_in_section(root_url)\n",
    "    \n",
    "    index = pi(db_filename=f'./persistent_index/{STORAGE_FILE}.sqlite', \n",
    "               table_name='product_links')\n",
    "    \n",
    "    unv_links = index.get_unvisited()\n",
    "    num_links = len(unv_links)\n",
    "\n",
    "    # Traverse links to scrape reviews\n",
    "            \n",
    "    for i, link in enumerate(unv_links):\n",
    "        \n",
    "        # Build link\n",
    "        link = link+'comments'\n",
    "        \n",
    "        # Show progress\n",
    "        print(f'\\rItem {i} / {num_links}, page 1 / {pages}, reviews: {len(uk_reviews)}', end='')\n",
    "        \n",
    "        # Fetch num pages to process\n",
    "        pages = get_number_of_pages(link)\n",
    "\n",
    "        for page in range(2, pages + 1):\n",
    "            print(f'\\rItem {i} / {num_links}, page {page} / {pages}, reviews: {len(uk_reviews)}', end='')\n",
    "            found_reviews = find_reviews(link, page)\n",
    "            \n",
    "            total_reviews.extend(found_reviews)\n",
    "            \n",
    "    ############# SAVE REVIEWS ############\n",
    "\n",
    "    with open('rozetka_mobile_reviews.json', 'w+') as fw:\n",
    "        json.dump(total_reviews, fw, ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "\n",
    "     \n",
    "# root_url = 'https://rozetka.com.ua/ua/mobile-phones/c80003/'\n",
    "root_url = 'https://rozetka.com.ua/notebooks/c80004/filter/'\n",
    "# root_url = 'https://rozetka.com.ua/protein/c273294/'\n",
    "\n",
    "all_links = set()\n",
    "uk_reviews = [] \n",
    "\n",
    "main0(root_url)\n",
    "\n",
    "# get_number_of_pages('https://rozetka.com.ua/ua/prestigio_psp7572duogold/p44474512/comments/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears that Rozetka DOES have a JSON API, but it's regrettably locked behind a \n",
    "# JWT security token generated client-side via obfuscated Javascript. Therefore, we \n",
    "# decided to revert to old-school scraping.\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "ref1 = 'https://retail.rozetka.com.ua/touch/?m[getCommentsByOffer]=%5B%7B%22offer_id%22%3A72016107%7D%2C108%2C36%5D'\n",
    "ref2 = 'https://retail.rozetka.com.ua/touch/?m[getCommentsByOffer]=%5B%7B%22offer_id%22%3A72016107%7D%2C36%2C36%5D'\n",
    "ref3 = 'https://retail.rozetka.com.ua/touch/?m[getCommentsByOffer]=%5B%7B%22offer_id%22%3A72016107%7D%2C72%2C36%5D'\n",
    "\n",
    "def decode(url):\n",
    "    return urllib.parse.unquote(url)\n",
    "\n",
    "def encode(url):\n",
    "    return urllib.parse.quote(url)\n",
    "\n",
    "def j_request(offer_id, val1, val2):\n",
    "    \n",
    "    offer_id = str(offer_id)\n",
    "    val1 = str(val1)\n",
    "    val2 = str(val2)\n",
    "    \n",
    "    headers = {\n",
    "        'User-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15',\n",
    "        'Referer': 'https://rozetka.com.ua/',\n",
    "        'Accept': 'application/vnd.retail-v4+json',\n",
    "        'Accept-Encoding': 'br, gzip, deflate',\n",
    "        'Host': 'retail.rozetka.com.ua',\n",
    "        'Origin' : 'https://m.rozetka.com.ua',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': f'https://m.rozetka.com.ua/comments/{offer_id}/1',\n",
    "        'X-Requested-With': 'XMLHttpRequest',\n",
    "        'Ajax-Referer': 'https://m.rozetka.com.ua/comments/72016107/2',\n",
    "        'Authorization': 'Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpc3MiOjYwMjE1MjksInN1YiI6MTIsImV4cCI6MTU1NDkyMDQ0NiwiZHQiOiJ0b3VjaCIsImp0aSI6IjllYjI1ZTJlMTJkYWUwYTA0MWVkNjA1MGZlNDU0MmMyMThmMTViOGEiLCJ1a2V5IjoiJDJ5JDEwJHEzbEJBNEhcL3hZU3JEYVVYUW83c1R1QnJwUFNUNjVSekNwUVwvZ2RJaHRVbVwvQUk0dFwvQXkxUyJ9.nMhS5hxO4ha8VfbwR6fS5wqLqM2DdnSwuZ6trNnAH13Y25n7aYVKQz11v_zTjAvxvObZFFPnzHjj7AxrSDTIRIEogYKgInmb8IefglpuqltWd7nktBiP1MGed2Rr_JEbporvw-8D4GLX2gMQvHHwU0nlU_gmNGaQ4Kyyuv34h6iM62-uHWxvHnNg3Xk0-jq_1xw-khMPfQD_Wz_tO2OjJIRAthxgh-yEE-uceg_jTg6ho0NC8Os8bUlZcKnGUAI3ovUyKQf8sOvXxcFOmqobIpRyOYRFcMTaU2ILxu9UoijK9idfva704O5VtsopSpiqf9yr5ucuvY8RimglWQdjcC1q1ECnYxN6uH3v29ZAj9o429rPWmv43gylI-3eWiCLCbIe6BtfCbM_lJMqW-YZjFnHGgqpQfs2ryEXi2_qAZSSWdV3py2sOe-q_d6qTWsH4kYWTJZc6tAG2fEW7W4aORYCuZailk0oGd6F2KaFNfLr6WjDFkCq2DWcfIgcDO3gRsJdr6BrODsC_-3n8tQnKoVmM7KC6SG0Ms8BxABsJUVGIeScPMgtSnq8IOHDBgKppE8T0qGzJXB5y0Ffq2AoOKYJ3RMk2xKiRRn9KTP60pKzv5kln3wRKtuWFbhieavbdeLEL2XIZUryK_khuzxUqXwK3wyrfHVF861GGRVX2mE'\n",
    "    }\n",
    "    \n",
    "    url = 'https://retail.rozetka.com.ua/touch/?m[getCommentsByOffer]=[{\"offer_id\":'+offer_id+'},'+val1+','+val2+']'\n",
    "        \n",
    "    # session = trident.init()\n",
    "    session = requests.session()\n",
    "    # session.headers.update(headers)\n",
    "    \n",
    "    # Run request\n",
    "    r = session.get(url, headers=headers)\n",
    "    return r.text\n",
    "    \n",
    "\n",
    "    \n",
    "assert ref1 == decode(encode(ref1))\n",
    "decode(ref2)\n",
    "\n",
    "# print(j_request(72016107, 36, 36))\n",
    "\n",
    "url = 'https://m.rozetka.com.ua/comments/73776633/'\n",
    "session = requests.session()\n",
    "r = session.get(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
