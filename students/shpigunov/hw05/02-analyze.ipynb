{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904688463911166\n"
     ]
    }
   ],
   "source": [
    "# Data access and preparation\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer as dv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import operator\n",
    "import tokenize_uk\n",
    "import pymorphy2\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "from persistent_index import PersistentKeyValueStorage as kvs\n",
    "\n",
    "\n",
    "def freq_analysis(comments):\n",
    "    \"\"\"Simple frequency analysis\"\"\"\n",
    "    \n",
    "    freq_dic = {}\n",
    "\n",
    "    for comment in comments:\n",
    "        for word in comment.split():\n",
    "            if word not in freq_dic:\n",
    "                freq_dic[word] = 1\n",
    "            else:\n",
    "                freq_dic[word] += 1\n",
    "\n",
    "    sorted_d = sorted(freq_dic.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    pp(sorted_d)\n",
    "\n",
    "    \n",
    "def generate_bows(comments, stop_words, morph):\n",
    "    \"\"\"Generate bags-of-words from comment texts\"\"\"\n",
    "    \n",
    "    bags = []\n",
    "\n",
    "    for comment in comments:\n",
    "        bag = {}\n",
    "        tokens = tokenize_uk.tokenize_words(comment)\n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            # Lemmatize\n",
    "            token = morph.parse(token)[0].normal_form.lower()\n",
    "            \n",
    "            if token not in stop_words:\n",
    "                if token not in bag:\n",
    "                    bag[token] = 1\n",
    "                else:\n",
    "                    bag[token] += 1\n",
    "        bags.append(bag)\n",
    "    \n",
    "    return bags\n",
    "\n",
    "\n",
    "def n_grams(tokens, n=1):\n",
    "    \"\"\"Returns an iterator over the n-grams given a list of tokens\"\"\"\n",
    "    shiftToken = lambda i: (el for j,el in enumerate(tokens) if j>=i)\n",
    "    shiftedTokens = (shiftToken(i) for i in range(n))\n",
    "    tupleNGrams = zip(*shiftedTokens)\n",
    "    return tupleNGrams # if join in generator : (\" \".join(i) for i in tupleNGrams)\n",
    "\n",
    "\n",
    "def range_ngrams(tokens, ngramRange=(1,2)):\n",
    "    \"\"\"Returns an itirator over all n-grams for n in range(ngramRange) given a list of tokens.\"\"\"\n",
    "    return chain(*(n_grams(tokens, i) for i in range(*ngramRange)))\n",
    "\n",
    "\n",
    "def bag_of_ngrams(comment, stop_words, morph, n=2):\n",
    "    \n",
    "    bag = {}\n",
    "    tokens = []\n",
    "    doc = tokenize_uk.tokenize_words(comment)\n",
    "    \n",
    "    # Lemmatize, normalize text\n",
    "    for token in doc:\n",
    "        lemma = morph.parse(token)[0].normal_form.lower()\n",
    "        if lemma not in stop_words:\n",
    "            tokens.append(lemma)\n",
    "        \n",
    "    for ngram in n_grams(tokens, n):\n",
    "        if ngram not in bag:\n",
    "            bag[ngram] = 1\n",
    "        else:\n",
    "            bag[ngram] += 1\n",
    "\n",
    "    return bag\n",
    "\n",
    "\n",
    "def generate_bags_of_ngrams(comments, stop_words, morph, n=2):\n",
    "    \"\"\"Generate bags-of-words from comment texts\"\"\"\n",
    "    return [bag_of_ngrams(comment, stop_words, morph, n=2) for comment in comments]\n",
    "\n",
    "\n",
    "def access_data(dataset_name):\n",
    "    \"\"\"Access data from storage and do very basic filtering\"\"\"\n",
    "    \n",
    "    storage = kvs(db_filename=f'./persistent_index/{dataset_name}.sqlite', \n",
    "                  table_name='kvs')   \n",
    "\n",
    "    eligible_scores = {5, 2, 1}\n",
    "\n",
    "    data = [entry for entry in storage.get_all() if entry[1] in eligible_scores]\n",
    "\n",
    "    comments = [entry[0] for entry in data]\n",
    "    scores = [entry[1] for entry in data]\n",
    "\n",
    "    return comments, scores\n",
    "\n",
    "\n",
    "def upsample_by_score(comments, scores, scores_to_upsample, multiplier=3):\n",
    "    \n",
    "    n = len(comments)\n",
    "    upsampled_comments = []\n",
    "    upsampled_scores = []\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        if scores[i] in scores_to_upsample:\n",
    "            for j in range(0, multiplier):\n",
    "                upsampled_comments.append(comments[i])\n",
    "                upsampled_scores.append(scores[i])\n",
    "        else:\n",
    "            upsampled_comments.append(comments[i])\n",
    "            upsampled_scores.append(scores[i])\n",
    "            \n",
    "    return upsampled_comments, upsampled_scores\n",
    "    \n",
    "\n",
    "# freq_analysis(comments)\n",
    "\n",
    "normalize_rules = {\n",
    "    '\\n\\n': '\\n',\n",
    "}\n",
    "\n",
    "stop_words = {\n",
    "    'Достоинства', \n",
    "    'Недостатки',\n",
    "    'і',\n",
    "    'з',\n",
    "    'на',\n",
    "    'в',\n",
    "    'що',\n",
    "    'як',\n",
    "    'то',\n",
    "    'у',\n",
    "    'це',\n",
    "    'для',\n",
    "    '.', ',', ')', '(', ':',\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch from storage\n",
    "comments, scores = access_data('phones')\n",
    "\n",
    "# Process comments into bags of words \n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "\n",
    "bags = generate_bows(comments, stop_words, morph)\n",
    "# bags = generate_bags_of_ngrams(comments, stop_words, morph, n=2)\n",
    "\n",
    "v = dv(sparse=False)\n",
    "X = v.fit_transform(bags)\n",
    "Y = []\n",
    "\n",
    "# Homogenize scores into two classes - positive (1) and negative(0)\n",
    "\n",
    "for score in scores:\n",
    "    if score <= 3:\n",
    "        Y.append(0)\n",
    "    if score == 5:\n",
    "        Y.append(1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=1)\n",
    "\n",
    "# Upsample negative tags\n",
    "scores_to_upsample = {0}\n",
    "mult = 5\n",
    "X_train, Y_train = upsample_by_score(X_train, Y_train, scores_to_upsample, multiplier=mult)\n",
    "X_test, Y_test = upsample_by_score(X_test, Y_test, scores_to_upsample, multiplier=mult)\n",
    "\n",
    "# Print ratio of negative and positive ratings\n",
    "print(sum(Y)/len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTree Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.41      0.53       540\n",
      "           1       0.74      0.92      0.82       962\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1502\n",
      "   macro avg       0.74      0.67      0.67      1502\n",
      "weighted avg       0.74      0.74      0.71      1502\n",
      "\n",
      "Gaussian Naive Bayes Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.24      0.30       540\n",
      "           1       0.65      0.79      0.71       962\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      1502\n",
      "   macro avg       0.52      0.52      0.51      1502\n",
      "weighted avg       0.56      0.59      0.56      1502\n",
      "\n",
      "kNN Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.08      0.15       540\n",
      "           1       0.65      0.97      0.78       962\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      1502\n",
      "   macro avg       0.62      0.53      0.46      1502\n",
      "weighted avg       0.63      0.65      0.55      1502\n",
      "\n",
      "LogReg Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.44      0.58       540\n",
      "           1       0.75      0.97      0.85       962\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1502\n",
      "   macro avg       0.82      0.70      0.72      1502\n",
      "weighted avg       0.80      0.78      0.75      1502\n",
      "\n",
      "SVM Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.19      0.31       540\n",
      "           1       0.68      0.99      0.81       962\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1502\n",
      "   macro avg       0.81      0.59      0.56      1502\n",
      "weighted avg       0.78      0.70      0.63      1502\n",
      "\n",
      "Perceptron Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.50      0.65       540\n",
      "           1       0.78      0.97      0.86       962\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1502\n",
      "   macro avg       0.84      0.74      0.75      1502\n",
      "weighted avg       0.83      0.80      0.79      1502\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "      fit_intercept=True, max_iter=100, n_iter=None, n_iter_no_change=5,\n",
       "      n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.01,\n",
       "      validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification and evaluation\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def dtree_classify(X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"Run classification with Decision trees\"\"\"\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('DTree Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    \n",
    "    return clf\n",
    "\n",
    "    \n",
    "def nbayes_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = GaussianNB()\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('Gaussian Naive Bayes Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "    \n",
    "def knn_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('kNN Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "def logreg_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                             multi_class='multinomial')\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('LogReg Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "def svm_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('SVM Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "def perceptron_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = Perceptron(max_iter=100, tol=1e-2)\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('Perceptron Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "dtree_classify(X_train, X_test, Y_train, Y_test)\n",
    "nbayes_classify(X_train, X_test, Y_train, Y_test)\n",
    "knn_classify(X_train, X_test, Y_train, Y_test)\n",
    "logreg_classify(X_train, X_test, Y_train, Y_test)\n",
    "svm_classify(X_train, X_test, Y_train, Y_test)\n",
    "perceptron_classify(X_train, X_test, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "* Larger and more diverse datasets guarantee better results. This is obvious, but still, it needs to be stated that smaller categories (proteins, laptops) yield worse model qaulity than larger ones (mobile phones).\n",
    "* Among all classifiers tested above, decision trees and logistic regression appeared to have the highest overall scores.\n",
    "* Experiments show that, counterintuitively, bags of words appear to perform better than collections of 2-grams. n-grams make learning and classification __much__ slower, though.\n",
    "* Again, counterintuitively, in our case, both lemmatization and stopword filtering have been somewhat effective for bags of __words__ and removing these processes from the pipeline decreased model scores.\n",
    "* Tweaking classifier parameters may be useful to improve model quality, but only up to a certain point.\n",
    "* Finally, it has been observed that upsamlping data by underrepresented tags __drastically__ improves recall and f1 for the underrepresented class. However, oversampling the same data negatively impacts quality for both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
