{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import OrderedDict as od\n",
    "from operator import itemgetter\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Global constants\n",
    "DEBUG = False\n",
    "HOSTNAME_PATTERN = \"\"\"(https?:\\/\\/)([^:^\\/]*)(:\\\\d*)?(.*)?\"\"\"\n",
    "\n",
    "filename = 'cdx-00000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_host_distribution(filename):\n",
    "    \"\"\"Count representation of host/domain names in a crawl index\n",
    "    Returns OrderedDict() of {host: num_occurrences}\"\"\"\n",
    "    \n",
    "    c = 0\n",
    "    hosts = {}\n",
    "    f = open(filename, 'r')\n",
    "    \n",
    "    for line in f:\n",
    "        json_string = line.split(' ', 2)[2]\n",
    "        json_data = json.loads(json_string)\n",
    "        \n",
    "        # Parse hostname from url\n",
    "        try:\n",
    "            hostname = re.findall(HOSTNAME_PATTERN, json_data['url'].lower())[0][1]\n",
    "        except IndexError:\n",
    "            print(\"Error extracting hostname from:\", json_data['url'].lower())\n",
    "        \n",
    "        # Tally hostnames\n",
    "        if hostname not in hosts:\n",
    "            hosts[hostname] = 1\n",
    "        else:\n",
    "            hosts[hostname] += 1\n",
    "        \n",
    "        # Debug limiter to test on smaller scope\n",
    "        if c > 100000 and DEBUG:\n",
    "            break\n",
    "        else:\n",
    "            c+=1\n",
    "    \n",
    "    # Order items\n",
    "    ordered_hosts = od(sorted(hosts.items(), key=itemgetter(1), reverse=True))\n",
    "        \n",
    "    return ordered_hosts\n",
    "\n",
    "hosts_json = json.dumps(count_host_distribution(f))\n",
    "with open('hosts_distr.json', 'w+') as json_file:\n",
    "    json_file.write(hosts_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"spa\": 5183261, \"eng\": 3639422, \"hye\": 533099, \"rus\": 367110, \"sqi\": '\n",
      " '261662, \"ara\": 219028, \"zho\": 157002, \"deu\": 131672, \"por\": 97808, \"jpn\": '\n",
      " '89145, \"fra\": 81097, \"ind\": 65002, \"cat\": 63185, \"ita\": 50238, \"tur\": 44430, '\n",
      " '\"fas\": 39098, \"lat\": 38556, \"nld\": 37310, \"dan\": 36754, \"tha\": 36742, \"srp\": '\n",
      " '32959, \"grn\": 27273, \"ron\": 21110, \"kor\": 20500, \"pus\": 18778, \"ukr\": 16855, '\n",
      " '\"pol\": 13823, \"tat\": 11032, \"ces\": 7618, \"uzb\": 7068, \"vie\": 7038, \"ina\": '\n",
      " '6576, \"oci\": 6490, \"hin\": 5840, \"bul\": 5329, \"war\": 5293, \"aar\": 5095, '\n",
      " '\"ell\": 4941, \"swe\": 4805, \"msa\": 4797, \"hun\": 4219, \"fin\": 4136, \"nor\": '\n",
      " '3969, \"kin\": 3864, \"nno\": 3639, \"aze\": 3173, \"mal\": 2794, \"hrv\": 2790, '\n",
      " '\"kat\": 2719, \"slk\": 2631, \"kal\": 2563, \"cos\": 2430, \"ile\": 2357, \"slv\": '\n",
      " '1974, \"lit\": 1732, \"bel\": 1730, \"heb\": 1643, \"vol\": 1550, \"est\": 1536, '\n",
      " '\"lin\": 1278, \"lav\": 1218, \"tam\": 1157, \"bos\": 1130, \"san\": 1089, \"mon\": '\n",
      " '1085, \"mkd\": 980, \"urd\": 930, \"eus\": 892, \"ben\": 891, \"sco\": 830, \"glg\": '\n",
      " '828, \"tgl\": 733, \"afr\": 712, \"jav\": 666, \"tuk\": 629, \"wol\": 578, \"xho\": 551, '\n",
      " '\"fry\": 474, \"roh\": 434, \"kan\": 432, \"haw\": 374, \"kaz\": 345, \"gle\": 344, '\n",
      " '\"kha\": 337, \"bak\": 337, \"epo\": 330, \"nep\": 316, \"bre\": 276, \"isl\": 269, '\n",
      " '\"khm\": 260, \"gla\": 256, \"bis\": 255, \"run\": 235, \"mar\": 218, \"ltz\": 214, '\n",
      " '\"hat\": 211, \"abk\": 206, \"hau\": 203, \"tsn\": 191, \"mfe\": 188, \"glv\": 181, '\n",
      " '\"ceb\": 181, \"mlg\": 180, \"kur\": 180, \"swa\": 173, \"sin\": 162, \"cym\": 143, '\n",
      " '\"sun\": 142, \"que\": 142, \"nya\": 140, \"som\": 134, \"tso\": 131, \"tgk\": 131, '\n",
      " '\"sot\": 130, \"ssw\": 130, \"tel\": 129, \"ven\": 124, \"crs\": 120, \"kir\": 113, '\n",
      " '\"mlt\": 111, \"ton\": 107, \"smo\": 106, \"zul\": 105, \"sna\": 91, \"syr\": 89, \"aka\": '\n",
      " '79, \"snd\": 74, \"lug\": 74, \"fao\": 73, \"yor\": 70, \"pan\": 70, \"nau\": 63, \"orm\": '\n",
      " '61, \"mya\": 55, \"mri\": 49, \"zha\": 49, \"fij\": 49, \"ipk\": 46, \"ibo\": 44, \"bih\": '\n",
      " '40, \"guj\": 34, \"uig\": 27, \"aym\": 23, \"chr\": 18, \"bod\": 17, \"lao\": 16, \"div\": '\n",
      " '12, \"blu\": 11, \"iku\": 10, \"nso\": 8, \"yid\": 7, \"sag\": 4, \"amh\": 4, \"tir\": 1, '\n",
      " '\"ori\": 1}')\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import requests\n",
    "\n",
    "# Suppress \"https error\" warning\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def count_langs(filename):\n",
    "    \"\"\"Count representation of languages in a crawl index\n",
    "    Returns OrderedDict() of {lang: num_occurrences}\"\"\"\n",
    "    \n",
    "    f = open(filename, 'r')\n",
    "    c = 0\n",
    "    langs = {}\n",
    "    \n",
    "    for line in f:\n",
    "        json_string = line.split(' ', 2)[2]\n",
    "        json_data = json.loads(json_string)\n",
    "        \n",
    "        # Parse hostname from url\n",
    "        url = json_data['url'].lower()\n",
    "        \n",
    "        # Ignore robots.txt\n",
    "        if '/robots.txt' in url:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            lang_list = json_data['languages'].lower().split(',')\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        \"\"\"\n",
    "        # Alternatively, languages can be detected directly\n",
    "        # by requesting a page and running a detection library \n",
    "        # on the page's text, but this has proven to be slower\n",
    "        # and less accurate\n",
    "        \n",
    "        # But it proves the concept that pages can be visited \n",
    "        # and analyzed individually\n",
    "        \n",
    "        lang_list = []\n",
    "        try:\n",
    "            r = requests.get(url, verify=False)\n",
    "        except:\n",
    "            print(\"HTTP Request Error:\", url)\n",
    "            continue\n",
    "        \n",
    "        soup = bs(r.text, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            lang = detect(soup.get_text())\n",
    "            lang_list.append(lang)\n",
    "        except LangDetectException:\n",
    "            print(\"Lang detect error:\", url)\n",
    "            continue        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Tally languages\n",
    "        for lang in lang_list:\n",
    "            if lang not in langs:\n",
    "                langs[lang] = 1\n",
    "            else:\n",
    "                langs[lang] += 1\n",
    "        \n",
    "        # Debug limiter and output\n",
    "        if c > 1000 and DEBUG:\n",
    "            break\n",
    "        else:\n",
    "            # print(c, url, lang)\n",
    "            c+=1\n",
    "    \n",
    "    # Order items\n",
    "    ordered_langs = od(sorted(langs.items(), key=itemgetter(1), reverse=True))\n",
    "        \n",
    "    return ordered_langs\n",
    "\n",
    "langs_json = json.dumps(count_langs(f))\n",
    "pp(langs_json)\n",
    "\n",
    "with open('langs_distr.json', 'w+') as json_file:\n",
    "    json_file.write(langs_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0,0,1)/ 20190222170014 {\"url\": \"http://1.000.000.000/\", \"mime\": \"text/html\", \"mime-detected\": \"text/html\", \"status\": \"403\", \"digest\": \"MRQTVY26B5MGVL2UCLQW6QWN7VMLMYCJ\", \"length\": \"1897\", \"offset\": \"21996\", \"filename\": \"crawl-data/CC-MAIN-2019-09/segments/1550247518497.90/crawldiagnostics/CC-MAIN-20190222155556-20190222181556-00457.warc.gz\"}\n",
      "\n",
      "0,0,0,1)/robots.txt 20190222170014 {\"url\": \"http://1.000.000.000/robots.txt\", \"mime\": \"text/html\", \"mime-detected\": \"text/html\", \"status\": \"403\", \"digest\": \"BTYSRH4M542AOCYIHRFCQ5CR7TBPGK2Y\", \"length\": \"1903\", \"offset\": \"2068\", \"filename\": \"crawl-data/CC-MAIN-2019-09/segments/1550247518497.90/robotstxt/CC-MAIN-20190222155556-20190222181556-00212.warc.gz\"}\n",
      "\n",
      "0,0,135,5)/ 20190218010049 {\"url\": \"http://5.135.0.0/\", \"mime\": \"text/html\", \"mime-detected\": \"text/html\", \"status\": \"200\", \"digest\": \"AH72W5G37N5O6ZJDCHMSIBVLFHZ4GQJR\", \"length\": \"18813\", \"offset\": \"2516608\", \"filename\": \"crawl-data/CC-MAIN-2019-09/segments/1550247483873.51/warc/CC-MAIN-20190217233327-20190218015327-00514.warc.gz\", \"charset\": \"UTF-8\", \"languages\": \"swe\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is an exploratory cell to study entry structure.\n",
    "f = open(filename, 'r')\n",
    "for i in range(0, 3):\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
