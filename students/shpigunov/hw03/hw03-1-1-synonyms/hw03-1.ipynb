{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Wiktionary Parsing\n",
    "\n",
    "Process a Wiktionary dump to extract synonym relations for a random language (not English, Ukrainian or Russian :). You can find the latest dumps at https://dumps.wikimedia.org/backup-index.html. The task requires application of XML SAX parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 5421/244834 – 2.2141532630271943%\n",
      "Contains {syn}: 9408 or 3.8426035599630772%\n"
     ]
    }
   ],
   "source": [
    "from xml.dom.minidom import parse, parseString\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "DEBUG = False\n",
    "IN_FILE = \"jawiktionary.xml\"\n",
    "OUT_FILE = \"jasynonyms.txt\"\n",
    "\n",
    "def parse_dom(in_file):\n",
    "    dom = parse(in_file)\n",
    "    pages = dom.getElementsByTagName(\"page\")\n",
    "    \n",
    "    return dom\n",
    "\n",
    "\n",
    "def parse_syn_section(s):\n",
    "    \"\"\"Parse wiktionary article content into sections.\n",
    "    \n",
    "    :rtype {'sec_name': 'sec_content' ...}\"\"\"\n",
    "    \n",
    "    patterns = [\n",
    "        r\"[={\\[]+syn[}\\]=]+\\n(.*?)[=]+[{]?\",\n",
    "        r\"[={\\[]+syn[}\\]=]+[：杼:=]?([ \\S]+)[\\n\\r$]+\",\n",
    "        r\"[={\\[]+syn[}\\]=]+[：杼:=]?(.*?)[=$]\"\n",
    "    ]\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        m = re.findall(pattern, s, re.M | re.S | re.U)\n",
    "        matches.extend(m)\n",
    "    \n",
    "    if matches:\n",
    "        return matches\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    \n",
    "def parse_synonyms(string_list):\n",
    "    \n",
    "    pattern = r\"\\[\\[(\\w+)\\]\\]\"\n",
    "    syns = set([])\n",
    "    \n",
    "    for s in string_list:\n",
    "        m = re.findall(pattern, s)\n",
    "        for item in m:\n",
    "            syns.add(item)\n",
    "    \n",
    "    return syns\n",
    "\n",
    "\n",
    "def contains_sec_header(s):\n",
    "    sec_header_pattern = r'{{(\\S+)}}'\n",
    "    \n",
    "    res = re.finditer(sec_header_pattern, s)\n",
    "    \n",
    "    if len(res) == 1:\n",
    "        return res[0]\n",
    "    elif len(res) > 1:\n",
    "        return [r for r in res]\n",
    "    elif len(res) < 1:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def contains_syn_header(s):\n",
    "    sec_header_pattern = r'{{syn}}'\n",
    "    \n",
    "    if len(re.findall(sec_header_pattern, s)) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def contains_ascii_chars(s, threshold=0):\n",
    "    \"\"\"Returns True if string contains more than `threshold` % \n",
    "    of ASCII chars (default=0 for no ASCII chars)\"\"\"\n",
    "    \n",
    "    chars = set(string.ascii_letters)\n",
    "    n = len(s)\n",
    "    m = 0\n",
    "    \n",
    "    for c in s:\n",
    "        if c in chars:\n",
    "            m+=1\n",
    "    \n",
    "    if m / n > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def analyze_synonyms(dom, out_file):\n",
    "\n",
    "    outfile = open(out_file, \"w+\")\n",
    "\n",
    "    art_total = 0\n",
    "    art_parsed = 0\n",
    "    art_contains_syn = 0\n",
    "\n",
    "    for page in pages:\n",
    "    \n",
    "        title = page.getElementsByTagName(\"title\")[0].firstChild.data\n",
    "        try:\n",
    "            text = page.getElementsByTagName(\"text\")[0].firstChild.data\n",
    "        except AttributeError:\n",
    "            if DEBUG:\n",
    "                print(\"Error: Article appears to have no text\")\n",
    "        art_total += 1\n",
    "        \n",
    "        # For any article that:\n",
    "        # has no ASCII chars in text (cut off non-Japanese words)\n",
    "        # has a {{syn}} section\n",
    "        \n",
    "        if not contains_ascii_chars(title) and contains_syn_header(text):\n",
    "            art_contains_syn += 1\n",
    "        \n",
    "            syns = parse_synonyms(parse_syn_section(text))\n",
    "            \n",
    "            # If article containing {{syn}} is parsed correctly\n",
    "            if len(syns) > 0:\n",
    "                art_parsed += 1\n",
    "                outfile.write(title)\n",
    "                outfile.write('\\n')\n",
    "                outfile.write(str(syns))\n",
    "                outfile.write('\\n\\n')\n",
    "                \n",
    "            # If the article has a section header but nothing is parsed\n",
    "            elif len(syns) == 0:\n",
    "                if DEBUG:\n",
    "                    print(title)\n",
    "                    print(text)\n",
    "                    \n",
    "        # Limiter for debug            \n",
    "        if DEBUG and art_total > 10000:\n",
    "            break\n",
    "\n",
    "    print(\"Parsed: {0}/{1} – {2}%\".format(art_parsed, art_total, art_parsed / art_total * 100))\n",
    "    print(\"Contains {{syn}}: {0} or {1}%\".format(art_contains_syn, art_contains_syn / art_total * 100))\n",
    "\n",
    "# dom = parse_dom(IN_FILE)\n",
    "analyze_synonyms(dom, OUT_FILE)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Спостереження\n",
    "\n",
    "Для парсингу XML ми надали перевагу моделі `minidom` над моделлю `SAX`, тому що перша виявилася набагато мінімалістичнішою та простішою у розгортанні за реалізацію `SAX` у мові `Python`. При цьому модель `minidom` також задовільно впоралася із отриманням тексту з `XML`-файлу.\n",
    "\n",
    "Щодо ефективності парсера тексту всередині статей, наш пасер покриває приблизно 2% всієї кількості статей у японському Wiktionary, що стосуються власне японських слів. При цьому:\n",
    "\n",
    "#### Всього статей: 244834\n",
    "\n",
    "#### Без відсікання не-японського тексту:\n",
    "\n",
    "Кількість із заголовком {{syn}}: 19002 або 7.76% від усіх статей\n",
    "\n",
    "Розпізнано парсером: 10271 або 4.195%\n",
    "\n",
    "#### З відсіканням латиничного тексту:\n",
    "\n",
    "Кількість із заголовком {{syn}}: 9408 або 3.84%\n",
    "\n",
    "Розпізнано парсером: 5421 або 2.21%\n",
    "\n",
    "## Висновки\n",
    "\n",
    "* У японському Wiktionary приблизно половина статей є допоміжними або такими, що не стосуються власне японських слів. У більш розвиненому парсері японські слова можна було б розрізняти за ознакою наявності у статті плашки `[[category:{{jpn}}]]`, але для швидкості ми обмежилися простим тестом на наявність латиничних символів;\n",
    "* Парсер покриває трохи більше половини всіх статей, що мають розділ `{{syn}}`. Відносно низький відсоток розпізнавання парсера обумовлений тим, що заголовки і вміст розділу із синонімами оформлений неоднорідно. Через це досить важко знайти універсальній набір фільтрів (рег.виразів), який би покривав усі можливі варіанти оформлення з однієї сторони, і не забирав би помилкво текст із неспоріднених розділів з іншої сторони. Позаяк парсер можна надалі покращувати за рахунок додавання нових і кормгування існуючих регулярних виразів.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
