{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Web Scraping\n",
    "\n",
    "Download and extract as separate texts all posts in a section of choice from http://forum.lvivport.com. The task requires web scraping.\n",
    "\n",
    "Hints: scrapy, readability, lazyweb, lazynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 64 threads.\n"
     ]
    }
   ],
   "source": [
    "# For debug purposes\n",
    "from pprint import pprint as pp\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import json\n",
    "\n",
    "base_url = \"http://forum.lvivport.com/\"\n",
    "\n",
    "# But of course we're parsing Divochi Posydenky\n",
    "board_url = \"http://forum.lvivport.com/forums/divochi-posidenki.194/\"\n",
    "\n",
    "\n",
    "def generate_urls(board_url, num_pages):\n",
    "    \"\"\"Generate urls for pagination\"\"\"\n",
    "    \n",
    "    urls = [board_url]\n",
    "    if num_pages > 1:\n",
    "        for i in range(2, num_pages+1):\n",
    "            urls.append(board_url + 'page-{0}'.format(i))\n",
    "        \n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_threads(board_url, num_pages):\n",
    "    \"\"\"Get thread ulrs from board index\"\"\"\n",
    "    \n",
    "    urls = generate_urls(board_url, num_pages)\n",
    "    threads = []\n",
    "    \n",
    "    for url in urls:\n",
    "        r = requests.get(url).text\n",
    "        soup = bs(r, 'html.parser')\n",
    "        \n",
    "        links = soup.select('ol.discussionListItems > li > div > div > h3 > a')\n",
    "        for link in links:\n",
    "            s = link.get('href')\n",
    "            threads.append(s)\n",
    "\n",
    "    return threads\n",
    "\n",
    "\n",
    "def get_content(base_url, threads):\n",
    "    \"\"\"\n",
    "    The actual content scraping routine\n",
    "    \n",
    "    Returns content in a nested dict as follows:\n",
    "    \n",
    "    threads = {int thread_id: {\n",
    "        int post_id1 : {\n",
    "            user: str,\n",
    "            text: str,\n",
    "        }\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # This will help clean up text in messages\n",
    "    text_postprocessors = {\n",
    "        '(adsbygoogle = window.adsbygoogle || []).push({});': '',\n",
    "        'Натисніть, щоб розгорнути...': '',\n",
    "        '\\n\\n\\n\\n': '\\n',\n",
    "        '\\n\\n\\n': '\\n',\n",
    "        '\\n\\n': '\\n',\n",
    "        '↑\\n': '',\n",
    "        '\t': '',\n",
    "        '  ': ' '\n",
    "    }\n",
    "    \n",
    "    # Init forum container\n",
    "    forum_content = {}\n",
    "    \n",
    "    # Walk each thread url\n",
    "    for thread_url in threads:\n",
    "        # Build url\n",
    "        url = base_url + thread_url\n",
    "                \n",
    "        # Request & soupify page\n",
    "        r = requests.get(url).text\n",
    "        soup = bs(r, 'html.parser')\n",
    "        \n",
    "        # Parse thread id\n",
    "        try:\n",
    "            thread_id = int(re.findall(r'\\.(\\d+)\\/', url)[0])\n",
    "        except IndexError:\n",
    "            print(\"Invalid Thread ID\")\n",
    "            thread_id = -1\n",
    "                \n",
    "        # Init thread container\n",
    "        thread_content = {}\n",
    "        \n",
    "        # Messages on the board live in an <ol> tag\n",
    "        # Parse the tag\n",
    "        messages = soup.select('ol.messageList')\n",
    "        \n",
    "        for tag in messages:\n",
    "            \n",
    "            # Posts live as <li> items\n",
    "            posts = tag.select('li')\n",
    "            \n",
    "            # for <li> in <ol>:\n",
    "            for post in posts:\n",
    "                \n",
    "                # Init post container\n",
    "                post_content = {}\n",
    "                                \n",
    "                # Parse post_id\n",
    "                try:\n",
    "                    txt = post.get('id')\n",
    "                    post_id = re.findall(r'post-(\\d+)', txt)[0]\n",
    "                except TypeError:\n",
    "                    # Some <li> will be non-content sections like ads, etc.\n",
    "                    continue\n",
    "\n",
    "                # Parse user_id\n",
    "                try:\n",
    "                    p = r'\\/(\\S+)\\/'\n",
    "                    txt = post.select('div.avatarHolder a')[0].get('href')\n",
    "                    user_id = re.findall(p, txt)[0]\n",
    "                    post_content['user_id'] = user_id\n",
    "\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "                # Parse post_text\n",
    "                try:\n",
    "                    # Extract text\n",
    "                    post_text = post.select('article')[0].get_text()\n",
    "\n",
    "                    # Run post-processors for text cleanup\n",
    "                    for pp in text_postprocessors.keys():\n",
    "                        post_text = post_text.replace(pp, text_postprocessors[pp])\n",
    "\n",
    "                    post_content['post_text'] = post_text\n",
    "\n",
    "                except IndexError:\n",
    "                    pass                \n",
    "                \n",
    "                # Save post into thread container\n",
    "                thread_content[post_id] = post_content\n",
    "\n",
    "        # Save thread into board container\n",
    "        forum_content[thread_id] = thread_content\n",
    "    \n",
    "    return forum_content\n",
    "    \n",
    "    \n",
    "\n",
    "threads = get_threads(board_url, 5)\n",
    "forum_content = get_content(base_url, threads)\n",
    "json_string = json.dumps(forum_content)\n",
    "\n",
    "jfile = open('posydenky.json', 'w+')\n",
    "jfile.write(json_string)\n",
    "jfile.close()\n",
    "\n",
    "print(\"Parsed %s threads.\" % len(threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>Подарунок хлопцю | Львівський Форум</title>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
