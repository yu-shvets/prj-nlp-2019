{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = ['say', 'tell', 'speak', 'claim', 'communicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 {'mouth', 'intercommunicate', 'communicate', 'enounce', 'address', 'tell', 'pronounce', 'severalize', 'convey', 'separate', 'exact', 'say', 'enunciate', 'claim', 'verbalize', 'secern', 'differentiate', 'verbalise', 'articulate', 'recite', 'take', 'assure', 'commune', 'distinguish', 'narrate', 'tell_apart', 'state', 'suppose', 'talk', 'arrogate', 'enjoin', 'transmit', 'evidence', 'sound_out', 'severalise', 'read', 'order', 'speak', 'secernate', 'allege', 'utter', 'recount', 'pass_on', 'aver', 'lay_claim', 'pass_along', 'pass', 'put_across'}\n"
     ]
    }
   ],
   "source": [
    "def get_synonims_lemmas(verbs):\n",
    "    synonims_lemmas = set()\n",
    "    for word in verbs:\n",
    "        #print(word, '---')\n",
    "        for syn in wn.synsets(word):\n",
    "            if (syn.pos() == 'v'):\n",
    "                synonims_lemmas = synonims_lemmas | set(syn.lemma_names())\n",
    "                #print(syn, syn.lemma_names(), syn.definition())\n",
    "                #pprint(syn.tree(lambda s:s.hypernyms()))\n",
    "    return synonims_lemmas\n",
    "print(len(get_synonims_lemmas(verbs)), get_synonims_lemmas(verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def child_ly_adverbs_in_sentence(sentence_doc, verb_lemma):\n",
    "    def get_child_ly_tokens(token):\n",
    "        return [child_token for child_token in token.children if child_token.pos_ == 'ADV' and child_token.text.endswith('ly')]\n",
    "\n",
    "    childs = []\n",
    "    for token in sentence_doc:\n",
    "        if token.pos_ == 'VERB' and token.lemma_ == verb_lemma:\n",
    "            child_ly_tokens = get_child_ly_tokens(token)\n",
    "            childs += [token.lemma_ for token in child_ly_tokens]\n",
    "            for token in child_ly_tokens:\n",
    "                for ch in token.children:\n",
    "                    if ch.dep_ == 'conj':\n",
    "                        childs += [ch.lemma_]\n",
    "\n",
    "    return childs\n",
    "\n",
    "assert child_ly_adverbs_in_sentence(nlp('say very Delightfully, tell quietly and seriously'), 'say') == ['delightfully']\n",
    "assert set(child_ly_adverbs_in_sentence(nlp('I speak quietly, seriously'), 'speak')) == set(['quietly', 'seriously'])\n",
    "assert set(child_ly_adverbs_in_sentence(nlp('He speaks calmly and confidently'), 'speak')) == set(['calmly', 'confidently'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2311.156805038452\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with open('blog2008.txt') as f:\n",
    "    blogs_text = f.readlines()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "verbs_lemmas = get_synonims_lemmas(verbs)\n",
    "#verbs_lemmas =  ['say', 'tell'] # 77seconds on blogs_text = blogs_text[:10000]\n",
    "stats = { verb : {} for verb in verbs_lemmas }# 77seconds on blogs_text = blogs_text[:10000]\n",
    "\n",
    "for doc in nlp.pipe(blogs_text, n_threads=16):\n",
    "    for stat_word in stats:\n",
    "        adverbs = child_ly_adverbs_in_sentence(doc, stat_word)\n",
    "        for adv in adverbs:\n",
    "            if adv in stats[stat_word]:\n",
    "                 stats[stat_word][adv] += 1\n",
    "            else:\n",
    "                 stats[stat_word][adv] = 1\n",
    "end = time.time()\n",
    "print('time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('all_stats.json', 'w') as f:\n",
    "    json.dump(stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_stats = {}\n",
    "for verb_and_stats in sorted(stats.items(), key=lambda kv: -len(kv[1])):\n",
    "    verb = verb_and_stats[0]\n",
    "    verb_stat = sorted(verb_and_stats[1].items(), key=lambda kv: -kv[1])[:10]\n",
    "    if len(verb_stat) > 4 and verb_stat[0][1] > 4:\n",
    "        short_stats[verb] = verb_stat\n",
    "\n",
    "with open('short_stats.json', 'w') as f:\n",
    "    json.dump(short_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
