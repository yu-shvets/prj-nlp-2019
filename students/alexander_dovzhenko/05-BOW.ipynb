{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze TV reviews in rozetka.ua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap all reviews and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "def scrap_rozetka(theme, page_num):\n",
    "    def scrap_items_urls(theme, page_num):\n",
    "        urls = []\n",
    "        #for page_num in range(1, pages_count + 1):\n",
    "        link = f'https://rozetka.com.ua/{theme}/page={page_num}'\n",
    "        f = requests.get(link)\n",
    "        for item_xml in BeautifulSoup(f.text).findAll('div', {'class': 'g-i-tile g-i-tile-catalog'}):\n",
    "            item_text_xml = item_xml.find('div', {'class': 'g-i-tile-i-title clearfix'})\n",
    "            urls += [item_text_xml.find('a')['href'] + '/comments']\n",
    "        return urls\n",
    "\n",
    "    def scrap_comments(comments_page_url):\n",
    "        f = requests.get(comments_page_url)\n",
    "        #print(len(BeautifulSoup(f.text).findAll('article', {'class': 'pp-review-i'})))\n",
    "        comment_texts_and_stars = []\n",
    "        for comment_xml in BeautifulSoup(f.text).findAll('article', {'class': 'pp-review-i'}):\n",
    "            comment_text = comment_xml.find('div', {'class': 'pp-review-text-i'})\n",
    "            stars_xml = comment_xml.find('span', {'class': 'sprite g-rating-stars-i'})\n",
    "            stars = int(stars_xml['content']) if stars_xml else -1\n",
    "            comment_texts_and_stars.append((comment_text.text, stars))\n",
    "        return comment_texts_and_stars\n",
    "\n",
    "    def scrap_pages_count(item_comments_link):\n",
    "        f = requests.get(item_comments_link)\n",
    "        all_buttons_xml = BeautifulSoup(f.text).find('ul', {'class': 'clearfix inline'})\n",
    "        if all_buttons_xml:\n",
    "            buttons_xmls = all_buttons_xml.findAll('li', {'class': 'paginator-catalog-l-i'})\n",
    "            if buttons_xmls:\n",
    "                last_button_xml = buttons_xmls[-1]\n",
    "                if last_button_xml['id']:\n",
    "                    return int(last_button_xml['id'].split()[-1])\n",
    "\n",
    "        return 1\n",
    "\n",
    "    all_comment_texts_and_stars = []\n",
    "    item_comments_urls = scrap_items_urls(theme, page_num)\n",
    "    for item_comments_link in item_comments_urls:\n",
    "        pages_count = scrap_pages_count(item_comments_link)\n",
    "        for page_num in range(pages_count):\n",
    "            comments_page_url = item_comments_link + f'/page={page_num}'\n",
    "            all_comment_texts_and_stars += scrap_comments(comments_page_url)\n",
    "\n",
    "    return all_comment_texts_and_stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>works 3-15 min/page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#for page_num in range(11, 30):\n",
    "for page_num in [1]:\n",
    "    try:\n",
    "        page_messanges_texts_and_ratings = scrap_rozetka('/all-tv/c80037/filter/', page_num)\n",
    "        with open(f'tv_comments_texts_and_ratings_page={page_num}.json', 'w') as f:\n",
    "            json.dump(page_messanges_texts_and_ratings, f)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_json = []\n",
    "for page_num in range(1, 30):\n",
    "    with open(f'tv_comments_texts_and_ratings_page={page_num}.json') as f:\n",
    "        all_comments_json += json.load(f)\n",
    "\n",
    "with open(f'all_tv_comments_texts_and_ratings.json', 'w') as f:\n",
    "    json.dump(all_comments_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select relevant reviews\n",
    "* has rating\n",
    "* UK language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import langid\n",
    "\n",
    "reviews_and_ratings = []\n",
    "with open(f'all_tv_comments_texts_and_ratings.json') as f:\n",
    "    reviews_and_ratings = json.load(f)\n",
    "\n",
    "reviews_and_ratings = [rr for rr in reviews_and_ratings if rr[1] >= 0]\n",
    "reviews_and_ratings = [rr for rr in reviews_and_ratings if langid.classify(rr[0])[0] == 'uk']\n",
    "with open(f'reviews_and_ratings.json', 'w') as f:\n",
    "    json.dump(reviews_and_ratings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 348, 2: 311, 3: 508, 4: 1742, 5: 6282}\n"
     ]
    }
   ],
   "source": [
    "rewiews_hist = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
    "for rr in reviews_and_ratings:\n",
    "    rewiews_hist[rr[1]] += 1\n",
    "print(rewiews_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess rewiews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tokenize_uk\n",
    "#!pip install -U https://github.com/kmike/pymorphy2/archive/master.zip#egg=pymorphy2\n",
    "#!pip install -U pymorphy2-dicts-uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize_uk\n",
    "import pymorphy2\n",
    "import re\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "\n",
    "assert morph.parse('встала')[0].normal_form == 'встати'\n",
    "assert tokenize_uk.tokenize_uk.tokenize_words('Це речення!') == ['Це', 'речення', '!']\n",
    "\n",
    "\n",
    "def text2norm_words(text):\n",
    "    words = tokenize_uk.tokenize_uk.tokenize_words(text)\n",
    "    words = [morph.parse(word)[0].normal_form for word in words]\n",
    "    words = [word for word in words if re.match(r'[\\w]+', word ,re.U)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "import random\n",
    "\n",
    "with open(f'reviews_and_ratings.json') as f:\n",
    "    reviews_and_ratings = json.load(f)\n",
    "\n",
    "random.shuffle(reviews_and_ratings)\n",
    "\n",
    "reviews_count = len(reviews_and_ratings)\n",
    "train_reviews_count = int(reviews_count * 0.7)\n",
    "train_reviews_and_ratings = reviews_and_ratings[:train_reviews_count]\n",
    "test_reviews_and_ratings = reviews_and_ratings[train_reviews_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline BOW classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(text):\n",
    "    norm_words = text2norm_words(text)\n",
    "    bow = {}\n",
    "    for word in norm_words:\n",
    "        if word not in bow:\n",
    "            bow[word] = 0\n",
    "        bow[word] += 1\n",
    "    return bow\n",
    "\n",
    "def stars_rating_to_sentiment_score(rating):\n",
    "    return {1:-1, 2:-1, 3:0, 4:1, 5:1}[rating]\n",
    "\n",
    "def get_hist(texts):\n",
    "    hist = {}\n",
    "    for text in texts:\n",
    "        for word in text2norm_words(text):\n",
    "            if not word in hist:\n",
    "                hist[word] = 1\n",
    "            hist[word] += 1\n",
    "    return hist\n",
    "\n",
    "def get_probs(texts):\n",
    "    hist = get_hist(texts)\n",
    "    total = len(texts)\n",
    "    return {item : (float(hist[item]) / total) for item in hist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "\n",
    "# Naive Bayess\n",
    "class BaselineBOWClassifier:\n",
    "    def __init__(self, train_reviews_and_ratings):\n",
    "        sent_classes = [-1, 0, 1]\n",
    "\n",
    "        classes_train_texts = {sent_class:[] for sent_class in sent_classes}\n",
    "        for rr in train_reviews_and_ratings:\n",
    "            sent_class = stars_rating_to_sentiment_score(rr[1])\n",
    "            classes_train_texts[sent_class].append(rr[0])\n",
    "\n",
    "        self.sent_classes = sent_classes\n",
    "        self.classes_probs = {sent_class : len(classes_train_texts[sent_class])/len(train_reviews_and_ratings) for sent_class in sent_classes}\n",
    "        self.words_probs = {sent_class : get_probs(classes_train_texts[sent_class]) for sent_class in sent_classes}\n",
    "\n",
    "    def get_class_probs(self, text):\n",
    "        def get_probs_sum(text, words_probs):\n",
    "            probs_sum = 0\n",
    "            for norm_word in get_bow(text):\n",
    "                if norm_word in words_probs:\n",
    "                    probs_sum += math.log(words_probs[norm_word])\n",
    "                # critical: beacuse log(0) - \n",
    "                else:\n",
    "                    probs_sum += math.log(1e-10)\n",
    "            return probs_sum\n",
    "\n",
    "        class_probs = {}\n",
    "        for sent_class in self.sent_classes:\n",
    "            class_probs[sent_class] = get_probs_sum(text, self.words_probs[sent_class]) + math.log(self.classes_probs[sent_class])\n",
    "\n",
    "        return class_probs\n",
    "\n",
    "    # return from -1, 0, 1\n",
    "    def classify(self, text):\n",
    "        return max(self.get_class_probs(text).items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tone-based BOW classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get correct tone words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line_str):\n",
    "    word = line_str.split('\\t')[0]\n",
    "    tone = int(line_str.split('\\t')[1].rstrip())\n",
    "    return (word, tone)\n",
    "\n",
    "with open('tone-dict-uk.tsv') as f:\n",
    "    words_and_tones = f.readlines()\n",
    "words_and_tones = list(map(parse_line, words_and_tones))\n",
    "#pprint(words_and_tones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(train_reviews_and_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some potential issues after review:\n",
    "* <b>Not presented in tone dict:</b> \n",
    "'задоволены', 'Небольшой, как раз для кухни. Звук очень хороший', 'дуже ростроїв звук(', 'Получив,що хотів'\n",
    "* <b>Not easy to judge by tone dict:</b>\n",
    "'проблем небуде?', 'жодних проблем', 'НЕ КУПУЙТЕ. Самсунг набагато краще.', 'Телевізор бомба!', ['Який рік випуску даного ТВ?', 5]\n",
    "* <b>Typical for positive:</b> \n",
    "'!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BaselineBOWClassifier(train_reviews_and_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All -> Relevant -> Correct -> Separates impossible states -> Separates correct state\n",
      "3442 -> 483 -> 292 -> 292 -> 168\n"
     ]
    }
   ],
   "source": [
    "def is_relevant(tone_tuple):\n",
    "    word, tone, neg_prob, neutral_prob, pos_prob = tone_tuple\n",
    "    return neg_prob + neutral_prob + pos_prob\n",
    "\n",
    "# P(neg comment|tone == -2) > P(neutral comment|tone == -2) and P(neg comment| tone == -2) > P(pos comment|tone == -2)\n",
    "# ...\n",
    "def is_correct(tone_tuple):\n",
    "    word, tone, neg_prob, neutral_prob, pos_prob = tone_tuple\n",
    "    if tone == -2:\n",
    "        return neg_prob > max(neutral_prob, pos_prob)\n",
    "    if tone == -1:\n",
    "        return max(neg_prob, neutral_prob) > pos_prob\n",
    "    if tone == 0:\n",
    "        return neutral_prob > max(neg_prob, pos_prob)\n",
    "    if tone == 1:\n",
    "        return max(pos_prob, neutral_prob) > neg_prob\n",
    "    if tone == 2:\n",
    "        return pos_prob > max(neutral_prob, neg_prob)\n",
    "\n",
    "def get_impossible_states(tone_tuple):\n",
    "    word, tone, neg_prob, neutral_prob, pos_prob = tone_tuple\n",
    "\n",
    "    res = []\n",
    "    if neg_prob == 0:\n",
    "        res.append(-1)\n",
    "    if neutral_prob == 0:\n",
    "        res.append(0)\n",
    "    if pos_prob == 0:\n",
    "        res.append(1)\n",
    "\n",
    "    return res\n",
    "\n",
    "def separates_correct_state(tone_tuple):\n",
    "    word, tone, neg_prob, neutral_prob, pos_prob = tone_tuple\n",
    "    if tone == -2:\n",
    "        return neutral_prob == 0 and pos_prob == 0\n",
    "    if tone == -1:\n",
    "        return neutral_prob == 0 and pos_prob == 0\n",
    "    if tone == 0:\n",
    "        return neg_prob == 0 and pos_prob == 0\n",
    "    if tone == 1:\n",
    "        return neutral_prob == 0 and neg_prob == 0\n",
    "    if tone == 2:\n",
    "        return neutral_prob == 0 and neg_prob == 0\n",
    "\n",
    "def print_tuples(tone_tuples):\n",
    "    for tone_tuple in tone_tuples:\n",
    "        word, tone, neg_prob, neutral_prob, pos_prob = tone_tuple\n",
    "        print(f'{word} tone:{tone} -1/0/1: ({round(neg_prob, 4)}, {round(neutral_prob, 4)}, {round(pos_prob, 4)})')\n",
    "\n",
    "words_tones_probs = []\n",
    "for wt in words_and_tones:\n",
    "    word = wt[0]\n",
    "    tone = wt[1]\n",
    "    neg_prob = classifier.words_probs[-1][word] if word in classifier.words_probs[-1] else 0\n",
    "    neutral_prob = classifier.words_probs[0][word] if word in classifier.words_probs[0] else 0\n",
    "    pos_prob = classifier.words_probs[1][word] if word in classifier.words_probs[1] else 0\n",
    "    words_tones_probs.append((word, tone, neg_prob, neutral_prob, pos_prob))\n",
    "\n",
    "relevant_words_tones_probs = [tone_tuple for tone_tuple in words_tones_probs if is_relevant(tone_tuple)]\n",
    "correct_words_tones_probs = [tone_tuple for tone_tuple in relevant_words_tones_probs if is_correct(tone_tuple)]\n",
    "impossible_states = {tone_tuple[0]:get_impossible_states(tone_tuple) for tone_tuple in correct_words_tones_probs}\n",
    "feature_words_tones_probs = [tone_tuple for tone_tuple in correct_words_tones_probs if separates_correct_state(tone_tuple)]\n",
    "\n",
    "print('All -> Relevant -> Correct -> Separates impossible states -> Separates correct state')\n",
    "print(f'{len(words_tones_probs)} -> {len(relevant_words_tones_probs)} -> {len(correct_words_tones_probs)} -> {len(impossible_states)} -> {len(feature_words_tones_probs)}')\n",
    "\n",
    "incorrect_words = [w for w in relevant_words_tones_probs if w not in correct_words_tones_probs]\n",
    "#pprint(impossible_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tone-based classifier on top of baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class ToneBasedBOWClassifier:\n",
    "    def get_avg_tone(self, text):\n",
    "        tone_words = 0\n",
    "        tone_sum = 0\n",
    "        for norm_word in get_bow(text):\n",
    "            if norm_word in self.correct_words_tones_probs:\n",
    "                tone_words += 1\n",
    "                tone_sum += self.correct_words_tones_probs[norm_word]\n",
    "\n",
    "        return tone_sum / tone_words if tone_words else None\n",
    "\n",
    "    def get_x(self, text, tone):\n",
    "        probs = self.baseline_classifier.get_class_probs(text)\n",
    "        return [probs[-1], probs[0], probs[1], tone]\n",
    "\n",
    "    def __init__(self, train_reviews_and_ratings, correct_words_tones_probs):\n",
    "        self.baseline_classifier = BaselineBOWClassifier(train_reviews_and_ratings)\n",
    "\n",
    "        self.correct_words_tones_probs = {wp[0]:wp[1] for wp in correct_words_tones_probs}\n",
    "\n",
    "        X, Y = [], []\n",
    "        for rr in train_reviews_and_ratings:\n",
    "            text = rr[0]\n",
    "            tone = self.get_avg_tone(text)\n",
    "            if tone != None:\n",
    "                X.append(self.get_x(text, tone))\n",
    "                Y.append(stars_rating_to_sentiment_score(rr[1]))\n",
    "\n",
    "        self.clf = LogisticRegression().fit(X, Y)\n",
    "\n",
    "    def classify(self, text):\n",
    "        tone = self.get_avg_tone(text)\n",
    "        if tone != None:\n",
    "            return self.clf.predict([self.get_x(text, tone)])\n",
    "        else:\n",
    "            return self.baseline_classifier.classify(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rating_predictor(predictor, test_set):\n",
    "    def rpf1(test_predictions, gt, class_name):\n",
    "        test_predictions_add_gt = list(zip(test_predictions, gt))\n",
    "        tp = test_predictions_add_gt.count((class_name, class_name))\n",
    "        if gt.count(class_name) == 0 or test_predictions.count(class_name) == 0:\n",
    "            return 0, 0, 0\n",
    "        rec = tp / gt.count(class_name)\n",
    "        prec = tp / test_predictions.count(class_name)\n",
    "        f1 = 2 * rec * prec / (rec + prec)\n",
    "        return round(rec, 2), round(prec, 2), round(f1, 2)\n",
    "\n",
    "    test_predictions = [predictor.classify(test_sample[0]) for test_sample in test_set]\n",
    "    gt = [stars_rating_to_sentiment_score(test_sample[1]) for test_sample in test_set]\n",
    "\n",
    "    print(' - Neg_score: ', rpf1(test_predictions, gt, -1))\n",
    "    print(' - Neutral_score: ', rpf1(test_predictions, gt, 0))\n",
    "    print(' - Positive_score: ', rpf1(test_predictions, gt, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline BOW calssifier: \n",
      " - Neg_score:  (0.96, 0.84, 0.9)\n",
      " - Neutral_score:  (0.88, 0.73, 0.8)\n",
      " - Positive_score:  (0.97, 0.99, 0.98)\n"
     ]
    }
   ],
   "source": [
    "print('Baseline BOW calssifier: ')\n",
    "evaluate_rating_predictor(BaselineBOWClassifier(train_reviews_and_ratings), test_reviews_and_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tone-based BOW calssifier: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sasha/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/sasha/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Neg_score:  (0.96, 0.85, 0.9)\n",
      " - Neutral_score:  (0.84, 0.82, 0.83)\n",
      " - Positive_score:  (0.98, 0.99, 0.98)\n"
     ]
    }
   ],
   "source": [
    "print('Tone-based BOW calssifier: ')\n",
    "evaluate_rating_predictor(ToneBasedBOWClassifier(train_reviews_and_ratings, correct_words_tones_probs), test_reviews_and_ratings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
