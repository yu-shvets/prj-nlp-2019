{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize George_Bernard_Shaw books in Wikipedia article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GT and the Wikipedia article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read groundtruth from dbpedia, save as 'books.json' and extract to books_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREFIX : <http://dbpedia.org/resource/>\n",
    "\n",
    "SELECT ?book \n",
    "\n",
    "WHERE {\n",
    "\n",
    "?book dbo:author :George_Bernard_Shaw .\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_books_gt(fname):\n",
    "    with open(fname) as f:\n",
    "        books_gt_json = json.load(f)\n",
    "\n",
    "    books_gt = []\n",
    "    for book_j in books_gt_json['results']['bindings']:\n",
    "        book_url_name = book_j['book']['value'].split('/')[-1]\n",
    "        book_words_gt = book_url_name.split('_')\n",
    "        # remove 'film)'' and '(play)'\n",
    "        book_words_gt = [word for word in book_words_gt if not any([ch in word for ch in ['(', ')']])]\n",
    "        book_gt = ' '.join(book_words_gt)\n",
    "        books_gt.append(book_gt)\n",
    "\n",
    "    return sorted(books_gt)\n",
    "\n",
    "books_gt = get_books_gt('04-books.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "\n",
    "\n",
    "link = 'https://en.wikipedia.org/wiki/George_Bernard_Shaw'\n",
    "page_xml = BeautifulSoup(requests.get(link).text)\n",
    "article_xml = page_xml.find('div', {'class': 'mw-parser-output'})\n",
    "\n",
    "# also removes books list <div class=\"refbegin columns references-column-width\"\n",
    "for tag in article_xml.find_all(['div', 'table', 'ul', 'blockquote']): \n",
    "    tag.decompose()\n",
    "\n",
    "article_text = article_xml.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try baseline solution without NLP \n",
    "Detect by 'i' tag and first upper letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_similar(name, names):\n",
    "    SIMILARITY_THRESHOLD = 3\n",
    "    for another_name in names:\n",
    "        if nltk.edit_distance(name, another_name) < SIMILARITY_THRESHOLD:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_dublicates(books_names):\n",
    "    filtered_books_names = []\n",
    "    for book_name in books_names:\n",
    "        if not has_similar(book_name, filtered_books_names):\n",
    "            filtered_books_names.append(book_name)\n",
    "    return filtered_books_names\n",
    "\n",
    "def detect_books_by_formatting(article_xml):\n",
    "    books_names = []\n",
    "    for i_xml in article_xml.findAll('i'):\n",
    "        books_names.append(i_xml.text)\n",
    "\n",
    "    #One book may have few entries with minor changes\n",
    "    books_names = remove_dublicates(books_names)\n",
    "\n",
    "    # Simple rule\n",
    "    books_names = [bn for bn in books_names if list(bn)[0].isupper()]\n",
    "\n",
    "    return sorted(books_names)\n",
    "\n",
    "books_detected_by_formatting = detect_books_by_formatting(article_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(detected_set, gt_set):\n",
    "    intersected_samples = [sample for sample in gt_set if has_similar(sample, detected_set)]\n",
    "    if not intersected_samples:\n",
    "        return 0,0,0\n",
    "\n",
    "    tp = len(intersected_samples)\n",
    "    recall = float(tp) / len(gt_set)\n",
    "    precission = float(tp) / len(detected_set)\n",
    "    f1 = 2 * recall * precission / ( recall + precission )\n",
    "\n",
    "    #print(tp, len(detected_set), len(gt_set))\n",
    "    #print(recall, precission, f1)\n",
    "\n",
    "    return round(recall, 2), round(precission, 2), round(f1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No-NLP score: rec:0.75, prec:0.39, f1:0.51\n",
      "Detect by <i> tag and first upper letter\n"
     ]
    }
   ],
   "source": [
    "rec, prec, f1 = score(books_detected_by_formatting, books_gt)\n",
    "print(f'No-NLP score: rec:{rec}, prec:{prec}, f1:{f1}')\n",
    "print('Detect by <i> tag and first upper letter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# remove '[...]' and '\\n' to avoid wrong tokenization like:\n",
    "# 'the Board of Censors.[281'\n",
    "article_text = re.sub(r'\\[.*?\\]', '', article_text)\n",
    "article_text = re.sub(r'\\n', '', article_text)\n",
    "\n",
    "doc = nlp(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER without post-processing: rec:0.34, prec:0.03, f1:0.06\n"
     ]
    }
   ],
   "source": [
    "ner_texts = [ent.text for ent in doc.ents]\n",
    "ner_texts = remove_dublicates(ner_texts)\n",
    "rec, prec, f1 = score(ner_texts, books_gt)\n",
    "print(f'NER without post-processing: rec:{rec}, prec:{prec}, f1:{f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply rules to Named Entities\n",
    "* filter by label\n",
    "* rules for NE string: len > 2, fisrt upper letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select labels\n",
    "This script shows that only labels: PERSON, ORG, WORK_OF_ART, GPE has recall > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_and_ents = {}\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ not in labels_and_ents:\n",
    "        labels_and_ents[ent.label_] = []\n",
    "    labels_and_ents[ent.label_].append(ent.text)\n",
    "for label in labels_and_ents:\n",
    "    rec, prec, f1 = score(labels_and_ents[label], books_gt)\n",
    "    print(f'{label}:  rec:{rec}, prec:{prec}, f1:{f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Named Entities(NE): 703\n",
      "NE with 4 selected tags: 413\n",
      "NE with selected tags and rules for NE string (len > 2, fisrt upper letter): 367\n"
     ]
    }
   ],
   "source": [
    "print('All Named Entities(NE):', sum([len(remove_dublicates(labels_and_ents[label])) for label in labels_and_ents]))\n",
    "\n",
    "person_ents = list(set(labels_and_ents['PERSON']))\n",
    "org_ents = list(set(labels_and_ents['ORG']))\n",
    "work_of_art_ents = list(set(labels_and_ents['WORK_OF_ART']))\n",
    "gpe_ents = list(set(labels_and_ents['GPE']))\n",
    "selected_ents = person_ents + org_ents + work_of_art_ents + gpe_ents\n",
    "print('NE with 4 selected labels:', len(remove_dublicates(selected_ents)))\n",
    "\n",
    "selected_ents = [ent for ent in selected_ents if len(ent) > 2]\n",
    "selected_ents = [ent for ent in selected_ents if list(ent)[0].isupper()]\n",
    "selected_ents = remove_dublicates(selected_ents)\n",
    "print('NE with 4 selected labels and rules for NE string (len > 2, fisrt upper letter):', len(selected_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER after pre-selection: rec:0.33, prec:0.05, f1:0.09\n"
     ]
    }
   ],
   "source": [
    "rec, prec, f1 = score(selected_ents, books_gt)\n",
    "print(f'NER after pre-selection: rec:{rec}, prec:{prec}, f1:{f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply rules to sentences with Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words that can be in the same sentence with NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_book_detected_by_formatting(sent):\n",
    "    for book_str in books_detected_by_formatting:\n",
    "        if book_str in sent.text:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def lemmas_hist(sent_list):\n",
    "    def get_hist(str_list):\n",
    "        str_hist = {}\n",
    "        for string in str_list:\n",
    "            if string in str_hist:\n",
    "                str_hist[string] += 1\n",
    "            else:\n",
    "                str_hist[string] = 1\n",
    "\n",
    "        return sorted(str_hist.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    words_list = []\n",
    "    for sent in sent_list:\n",
    "        words_list += [token.lemma_ for token in sent]\n",
    "    return get_hist(words_list)\n",
    "    \n",
    "pos_sentences = [sent for sent in doc.sents if has_book_detected_by_formatting(sent)]\n",
    "print(lemmas_hist(pos_sentences)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually selected from printed list\n",
    "keywords = ['shaw', '(', 'play', 'write', 'which', 'work', 'become', 'publish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containes all rules from 'Apply rules to Named Entities'\n",
    "def check_ent(ent):\n",
    "    check_label = lambda ent : ent.label_ in ['PERSON', 'ORG', 'WORK_OF_ART', 'GPE']\n",
    "    check_text = lambda ent : (len(ent.text) > 2) and (list(ent.text)[0].isupper())\n",
    "    return check_label(ent) and check_text(ent)\n",
    "\n",
    "def check_sent(sent, keywords):\n",
    "    for token in sent:\n",
    "        if token.lemma_ in keywords:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_recognized_books = remove_dublicates([ent.text for ent in doc.ents if (check_ent(ent) and check_sent(ent.sent, keywords))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER after applying extra rules to it's sentence: rec:0.31, prec:0.06, f1:0.1\n"
     ]
    }
   ],
   "source": [
    "rec, prec, f1 = score(ner_recognized_books, books_gt)\n",
    "print(f'NER after applying extra rules to it\\'s sentence: rec:{rec}, prec:{prec}, f1:{f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
