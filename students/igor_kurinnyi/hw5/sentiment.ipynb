{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import jsonlines\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize_uk\n",
    "import pymorphy2\n",
    "import langdetect\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "stop_words = set(x.strip() for x in open('ukr_stopwords.txt').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd() / 'rozetka/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jl_file(data_file):\n",
    "    with jsonlines.open(data_file, 'r') as f:\n",
    "        for x in f:\n",
    "            yield x\n",
    "\n",
    "def write_data_lang(data_file):\n",
    "    out_file = f'{data_file.parent}/{data_file.stem}-lang{data_file.suffix}'\n",
    "    \n",
    "    with jsonlines.open(out_file, 'w') as f:\n",
    "        for comment in tqdm(jl_file(data_file)):\n",
    "            try:\n",
    "                lang = langdetect.detect(comment['text'])\n",
    "            except langdetect.detector.LangDetectException:\n",
    "                lang = ''\n",
    "            else:\n",
    "                comment['lang'] = lang\n",
    "                f.write(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 56.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/igor/projects/prj-nlp-2019/students/igor_kurinnyi/hw5/rozetka/data/kupanie-i-gigiena.jl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4233it [00:24, 176.07it/s]\n",
      "10it [00:00, 97.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/home/igor/projects/prj-nlp-2019/students/igor_kurinnyi/hw5/rozetka/data/tehnika-dlya-kuhni.jl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28088it [02:55, 159.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for f in DATA_DIR.iterdir():\n",
    "    if not str(f).endswith('-lang.jl'):\n",
    "        print(f)\n",
    "        write_data_lang(f)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    data = defaultdict(list)\n",
    "    for comment in jl_file(data_file):\n",
    "        if comment['rating'] and (comment['lang'] == 'uk'):\n",
    "            text = comment['text']\n",
    "            adv = get_advantages(comment['text'])\n",
    "            dis = get_disadvantages(comment['text'])\n",
    "            data['text'].append(text)\n",
    "            data['advantages'].append(adv)\n",
    "            data['disadvantages'].append(dis)\n",
    "            \n",
    "            data['rating'].append(comment['rating'])\n",
    "            data['like'].append(comment['likes'])\n",
    "            data['dislike'].append(comment['dislikes'])\n",
    "            data['author'].append(comment['author'])\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def get_advantages(text):\n",
    "    for par in text.split('\\n'):\n",
    "        if par.startswith('Достоинства'):\n",
    "            par = re.sub(r'Достоинства:?\\s?', '', par)\n",
    "            return par\n",
    "    return ''\n",
    "\n",
    "        \n",
    "def get_disadvantages(text):\n",
    "    for par in text.split('\\n'):\n",
    "        if par.startswith('Недостатки'):\n",
    "            par = re.sub(r'Недостатки:?\\s?', '', par)\n",
    "            return par\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>advantages</th>\n",
       "      <th>disadvantages</th>\n",
       "      <th>author</th>\n",
       "      <th>like</th>\n",
       "      <th>dislike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>В принципі нічого, каву варить, але надто поту...</td>\n",
       "      <td>Поки що працює</td>\n",
       "      <td>Традиційно для Розетки незаповнена гарантійка....</td>\n",
       "      <td>Віталій Бочковський</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>Дуже корисна річ! Всім рекомендую.\\nНедостатки...</td>\n",
       "      <td></td>\n",
       "      <td>Не має.</td>\n",
       "      <td>Люда Ященко</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>Дуже класний помічник!! зі всіма завданнями сп...</td>\n",
       "      <td>Легкий, дешевий</td>\n",
       "      <td>не виявила</td>\n",
       "      <td>Ольга Кулик</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Відмінна якість. Тости прожарюються рівномірно...</td>\n",
       "      <td>Автоматичний підйом тостів, автоцентрування, п...</td>\n",
       "      <td>Поки-що не знайшли.</td>\n",
       "      <td>Виктор</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>Працює) дуже гарна підсвітка! Якість, на свої ...</td>\n",
       "      <td>Працює) дуже гарна підсвітка! Якість, на свої ...</td>\n",
       "      <td></td>\n",
       "      <td>оксана</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1206  В принципі нічого, каву варить, але надто поту...   \n",
       "2592  Дуже корисна річ! Всім рекомендую.\\nНедостатки...   \n",
       "1143  Дуже класний помічник!! зі всіма завданнями сп...   \n",
       "35    Відмінна якість. Тости прожарюються рівномірно...   \n",
       "1613  Працює) дуже гарна підсвітка! Якість, на свої ...   \n",
       "\n",
       "                                             advantages  \\\n",
       "1206                                     Поки що працює   \n",
       "2592                                                      \n",
       "1143                                    Легкий, дешевий   \n",
       "35    Автоматичний підйом тостів, автоцентрування, п...   \n",
       "1613  Працює) дуже гарна підсвітка! Якість, на свої ...   \n",
       "\n",
       "                                          disadvantages               author  \\\n",
       "1206  Традиційно для Розетки незаповнена гарантійка....  Віталій Бочковський   \n",
       "2592                                            Не має.          Люда Ященко   \n",
       "1143                                         не виявила          Ольга Кулик   \n",
       "35                                  Поки-що не знайшли.               Виктор   \n",
       "1613                                                                  оксана   \n",
       "\n",
       "      like  dislike  \n",
       "1206     2        0  \n",
       "2592     0        0  \n",
       "1143     0        0  \n",
       "35       0        0  \n",
       "1613     0        0  "
      ]
     },
     "execution_count": 1046,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = DATA_DIR / 'tehnika-dlya-kuhni-lang.jl'\n",
    "data = load_data(data_file)\n",
    "\n",
    "target = ['rating']\n",
    "feature_names = ['text', 'advantages', 'disadvantages', 'author', 'like', 'dislike']\n",
    "data_train, data_test, y_train, y_test = train_test_split(data[feature_names], data[target], test_size=0.3, random_state=42, stratify=data[target])\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Текстові фічі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key, to_values=False):\n",
    "        self.key = key\n",
    "        self.to_values = to_values\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, df):\n",
    "        if self.to_values:\n",
    "            return df[self.key].values[:, None]\n",
    "        else:\n",
    "            return df[self.key]\n",
    "\n",
    "\n",
    "class TextProcessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lemma=True, no_stop_words=True, clean=True):\n",
    "        self.lemma = lemma\n",
    "        self.no_stop_words = no_stop_words\n",
    "        self.clean = clean\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        if self.lemma:\n",
    "            x = x.apply(self.lemmatize)\n",
    "        if self.no_stop_words:\n",
    "            x = x.apply(self.remove_stop_words)\n",
    "        if self.clean:\n",
    "            x = x.apply(self.clean_text)\n",
    "        return x\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        tokens = [t for t in tokenize_uk.tokenize_words(text)]\n",
    "        lemmas = [morph.parse(t)[0].normal_form for t in tokens]\n",
    "        return ' '.join(lemmas)\n",
    "    \n",
    "    def remove_stop_words(self, text):\n",
    "        return ' '.join([t for t in tokenize_uk.tokenize_words(text) if t not in stop_words])\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        sub_patterns = [\n",
    "            (r'\\+\\++', 'manyplus'),\n",
    "            (r'\\!+', ' manyexclamation '),\n",
    "            (r'\\?+', ' manyquest '),\n",
    "            (r'(\\?\\!)+', ' exclquest '),\n",
    "            (r'\\.\\.+', ' manydot '),\n",
    "            (r'[;:|]?-?\\)+', ' happysmile '),\n",
    "            (r'[;:|]?\\*?-?\\)+', ' sadsmile '),\n",
    "            (r\"(\\w+)'(\\w)\", r'\\g<1>\\g<2>'),\n",
    "            (r'\\W+', ' '),\n",
    "        ]\n",
    "        for p, s in sub_patterns:\n",
    "            text = re.sub(p, s, text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== TEXT FEATURES ======================\n",
    "\n",
    "def text_feature_pipe(field):\n",
    "    return Pipeline([\n",
    "        ('selector', FeatureSelector(field)),\n",
    "        ('processor', TextProcessor()),\n",
    "        ('tfidf', TfidfVectorizer()),])\n",
    "\n",
    "body = text_feature_pipe('text')\n",
    "adv = text_feature_pipe('advantages')\n",
    "dis = text_feature_pipe('disadvantages')\n",
    "\n",
    "text = FeatureUnion([\n",
    "    ('body', body),\n",
    "    ('adv', adv),\n",
    "    ('dis', dis),\n",
    "])\n",
    "\n",
    "mnb_pipe = Pipeline([\n",
    "    ('features', text),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# параметри вже підібрав\n",
    "text_params = {\n",
    "    'features__body__processor__lemma': [False],\n",
    "    'features__body__processor__no_stop_words': [True],\n",
    "    'features__body__processor__clean': [True],\n",
    "    'features__body__tfidf__ngram_range': [(1, 3)],\n",
    "    'features__body__tfidf__max_df': [1.0],\n",
    "    \n",
    "    'features__adv__processor__lemma': [True],\n",
    "    'features__adv__processor__no_stop_words': [True],\n",
    "    'features__adv__processor__clean': [True],\n",
    "    'features__adv__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__adv__tfidf__max_df': [0.7],\n",
    "    \n",
    "    'features__dis__processor__lemma': [True],\n",
    "    'features__dis__processor__no_stop_words': [True],\n",
    "    'features__dis__processor__clean': [True],\n",
    "    'features__dis__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__dis__tfidf__max_df': [0.7],\n",
    "}\n",
    "\n",
    "mnb_params = {'mnb__alpha': [0]}\n",
    "mnb_params.update(text_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.554\n",
      "Mean Score [0.55365155]\n",
      "STD  [0.00809414]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.13      0.21        39\n",
      "           2       0.00      0.00      0.00        31\n",
      "           3       0.25      0.04      0.07        46\n",
      "           4       0.29      0.16      0.20       164\n",
      "           5       0.68      0.91      0.78       522\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       802\n",
      "   macro avg       0.37      0.25      0.25       802\n",
      "weighted avg       0.55      0.63      0.57       802\n",
      "\n",
      "CPU times: user 4.66 s, sys: 15.8 ms, total: 4.67 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = GridSearchCV(mnb_pipe, mnb_params, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=2)\n",
    "clf.fit(data_train, y_train)\n",
    "\n",
    "print(f'Best Score: {clf.best_score_:.3f}')\n",
    "print('Mean Score', clf.cv_results_['mean_test_score'])\n",
    "print('STD ', clf.cv_results_['std_test_score'])\n",
    "print('\\n')\n",
    "\n",
    "y_predict = clf.predict(data_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Числові признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        result = list()\n",
    "        for row in X[['text', 'advantages', 'disadvantages']].itertuples():\n",
    "            len_text = len(re.split(r'\\W+', row.text))\n",
    "            len_adv = len(re.split(r'\\W+', row.advantages))\n",
    "            len_dis = len(re.split(r'\\W+', row.disadvantages))\n",
    "            stats = {\n",
    "                'len_text': len_text,\n",
    "                'len_adv': len_adv,\n",
    "                'len_dis': len_dis,\n",
    "                'adv_text': len_adv / len_text,\n",
    "                'dis_text': len_dis / len_text,\n",
    "                'adv_dis': len_adv / (len_dis + 0.001),\n",
    "                'dis_adv': len_dis / (len_adv + 0.01),\n",
    "                'is_latin': bool(re.search(r'\\b[A-Za-z]+\\b', row.text)),\n",
    "                'is_hastag': bool(re.search(r'#\\w+', row.text)),  # #моєрозпакування\n",
    "                'is_caps_text': bool(re.search(r'\\b[А-Я][А-Я]+\\b', row.text)),\n",
    "                'is_caps_text_latin': bool(re.search(r'\\b[A-Z][A-Z]+\\b', row.text)),\n",
    "                'is_caps_adv': bool(re.search(r'\\b[А-Я][А-Я]+\\b', row.advantages)),\n",
    "                'is_caps_dis': bool(re.search(r'\\b[А-Я][А-Я]+\\b', row.disadvantages)),\n",
    "                'quest': (len_adv + len_dis == 0) and (row.text.endswith('?'))  # питання (а не відгуки) в яких часто стоїть 5 зірок\n",
    "            }\n",
    "            result.append(stats)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorType(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return list(X['author'].apply(self.process_author))\n",
    "    \n",
    "    def process_author(self, author):\n",
    "        return {\n",
    "            'is_guest': author == 'Гость',\n",
    "            'is_cyrillic': bool(re.match(r'(\\p{IsCyrillic}+\\s?)+', author)),\n",
    "            'is_latin': not bool(re.match(r'(\\p{IsCyrillic}+\\s?)+', author)),\n",
    "            'n_words': len(re.split(r'\\W+', author)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tones():\n",
    "    tones = dict()\n",
    "    with open('tone-dict-uk.tsv', 'r') as f:\n",
    "        for line in f:\n",
    "            word, tone = line.split('\\t')\n",
    "            tones[word] = int(tone.strip())\n",
    "    return tones\n",
    "\n",
    "TONES = load_tones()\n",
    "TONES['manyexclamation'] = 2\n",
    "TONES['manyplus'] = 2\n",
    "TONES['manyquest'] = -2\n",
    "TONES['exclquest'] = -1\n",
    "TONES['manydot'] = -1\n",
    "TONES['happysmile'] = 2\n",
    "TONES['sadsmile'] = -2\n",
    "TONES['ок'] = 1\n",
    "TONES['окей'] = 1\n",
    "\n",
    "\n",
    "class ToneFeature(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return list(x.apply(lambda text: self.tone_stats(text)))\n",
    "    \n",
    "    def tone_stats(self, text):\n",
    "        scores = self.sent_to_tones(text)\n",
    "        pos_scores = [s for s in scores if s > 0]\n",
    "        neg_scores = [s for s in scores if s < 0]\n",
    "        return {\n",
    "            'min_tone': min(scores),\n",
    "            'max_tone': max(scores),\n",
    "            'max_to_min': abs(max(scores) / (min(scores) + 0.001)),\n",
    "            'tone_per_word': sum(scores) / len(scores),\n",
    "            'mean_tone': sum(scores) / (len(pos_scores) + len(neg_scores) + 0.001),\n",
    "            'neg_tone_per_word': abs(sum(neg_scores)) / len(scores),\n",
    "            'pos_tone_per_word': sum(pos_scores) / len(scores),\n",
    "            'mean_neg': abs(sum(neg_scores)) / (len(neg_scores) + 0.001),\n",
    "            'mean_pos': sum(pos_scores) / (len(pos_scores) + 0.001),\n",
    "            'pos_to_neg': sum(pos_scores) / (abs(sum(neg_scores)) + 0.001),\n",
    "        }\n",
    "    \n",
    "    def sent_to_tones(self, text):\n",
    "        tones = list()\n",
    "        tokens = tokenize_uk.tokenize_words(text)\n",
    "        for i, t in enumerate(tokens):\n",
    "            tone = TONES.get(t.lower(), 0)\n",
    "            if i > 0:\n",
    "                if tokens[i - 1] == 'не':\n",
    "                    tone *= -1\n",
    "            if i > 1:\n",
    "                prev = ' '.join(tokens[i - 2: i])\n",
    "                if re.match('не (дуже|зовсім|такий)', prev):\n",
    "                    tone *= -1\n",
    "            tones.append(tone)\n",
    "        return tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== NUMERICAL FEATURES =============================\n",
    "\n",
    "text_stats = Pipeline([\n",
    "    ('stats', TextStats()),\n",
    "    ('vect', DictVectorizer(sparse=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "\n",
    "tone_features = Pipeline([\n",
    "    ('selector', FeatureSelector('text')),\n",
    "    ('processor', TextProcessor(lemma=True)),\n",
    "    ('tone', ToneFeature()),\n",
    "    ('vect', DictVectorizer(sparse=False)),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "author_features = Pipeline([\n",
    "    ('stats', AuthorType()),\n",
    "    ('vect', DictVectorizer(sparse=False)),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "numerical = FeatureUnion([\n",
    "    ('text_stats', text_stats),\n",
    "    ('tone', tone_features),\n",
    "    ('author', author_features),\n",
    "    ('like', FeatureSelector('like', to_values=True)),\n",
    "    ('dislike', FeatureSelector('dislike', to_values=True))\n",
    "])\n",
    "\n",
    "logit_pipe = Pipeline([\n",
    "    ('numerical', numerical),\n",
    "    ('logit', LogisticRegression())\n",
    "])\n",
    "\n",
    "logit_params = {\n",
    "    'logit__C': [1000],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:   31.4s remaining:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   47.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.556\n",
      "Mean Score [0.55648293 0.55648293]\n",
      "STD  [0.0048353 0.0048353]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.23      0.32        39\n",
      "           2       0.00      0.00      0.00        31\n",
      "           3       1.00      0.02      0.04        46\n",
      "           4       0.42      0.10      0.16       164\n",
      "           5       0.69      0.98      0.81       522\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       802\n",
      "   macro avg       0.53      0.27      0.27       802\n",
      "weighted avg       0.62      0.67      0.58       802\n",
      "\n",
      "CPU times: user 13.9 s, sys: 324 ms, total: 14.3 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = GridSearchCV(logit_pipe, logit_params, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=2)\n",
    "clf.fit(data_train, y_train)\n",
    "\n",
    "print(f'Best Score: {clf.best_score_:.3f}')\n",
    "print('Mean Score', clf.cv_results_['mean_test_score'])\n",
    "print('STD ', clf.cv_results_['std_test_score'])\n",
    "print('\\n')\n",
    "\n",
    "y_predict = clf.predict(data_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комбінація текстових та числових признаків"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = FeatureUnion([\n",
    "    ('text', text),\n",
    "    ('numerical', numerical),\n",
    "])\n",
    "\n",
    "logit_pipe = Pipeline([\n",
    "    ('features', features),\n",
    "    ('logit', LogisticRegression())\n",
    "])\n",
    "\n",
    "mnb_pipe = Pipeline([\n",
    "    ('features', features),\n",
    "    ('logit', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_params = {\n",
    "    'features__text__body__processor__lemma': [False],\n",
    "    'features__text__body__processor__no_stop_words': [True],\n",
    "    'features__text__body__processor__clean': [True],\n",
    "    'features__text__body__tfidf__ngram_range': [(1, 3)],\n",
    "    'features__text__body__tfidf__max_df': [1.0],\n",
    "    \n",
    "    'features__text__adv__processor__lemma': [True],\n",
    "    'features__text__adv__processor__no_stop_words': [True],\n",
    "    'features__text__adv__processor__clean': [True],\n",
    "    'features__text__adv__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__text__adv__tfidf__max_df': [0.7],\n",
    "    \n",
    "    'features__text__dis__processor__lemma': [True],\n",
    "    'features__text__dis__processor__no_stop_words': [True],\n",
    "    'features__text__dis__processor__clean': [True],\n",
    "    'features__text__dis__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__text__dis__tfidf__max_df': [0.7],\n",
    "}\n",
    "\n",
    "logit_params = {\n",
    "    'logit__C': [1000]\n",
    "}\n",
    "logit_params.update(text_params)\n",
    "\n",
    "mnb_params = dict()\n",
    "mnb_params.update(text_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   38.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.622\n",
      "Mean Score [0.62214829]\n",
      "STD  [0.00844017]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.26      0.32        39\n",
      "           2       0.20      0.06      0.10        31\n",
      "           3       0.31      0.20      0.24        46\n",
      "           4       0.31      0.21      0.25       164\n",
      "           5       0.75      0.89      0.81       522\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       802\n",
      "   macro avg       0.40      0.32      0.34       802\n",
      "weighted avg       0.59      0.65      0.61       802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(logit_pipe, logit_params, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=2)\n",
    "\n",
    "clf.fit(data_train, y_train)\n",
    "\n",
    "print(f'Best Score: {clf.best_score_:.3f}')\n",
    "print('Mean Score', clf.cv_results_['mean_test_score'])\n",
    "print('STD ', clf.cv_results_['std_test_score'])\n",
    "print('\\n')\n",
    "\n",
    "y_predict = clf.predict(data_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест на категорії купання та гігієна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.40      0.57        10\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00        12\n",
      "           4       0.22      0.09      0.12        47\n",
      "           5       0.89      0.98      0.94       494\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       565\n",
      "   macro avg       0.42      0.29      0.33       565\n",
      "weighted avg       0.82      0.87      0.84       565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kupanie = load_data(DATA_DIR / 'kupanie-i-gigiena-lang.jl')\n",
    "\n",
    "kup_predict = clf.predict(kupanie[feature_names])\n",
    "report = classification_report(kupanie[target], kup_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стекінг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator, cv):\n",
    "        self.estimator = estimator\n",
    "        self.cv = cv\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.y_pred = cross_val_predict(self.estimator, X, y, cv=self.cv, method='predict_proba', verbose=0, n_jobs=-1)\n",
    "        self.estimator.fit(X, y)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        if getattr(self, 'y_pred', None) == None:\n",
    "            self.fit(X, y)\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoText(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.apply(self.filter_tones)\n",
    "        return X\n",
    "    \n",
    "    def filter_tones(self, text):\n",
    "        return ' '.join([t for t in tokenize_uk.tokenize_words(text) if t.lower() in TONES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== TEXT FEATURES ======================\n",
    "\n",
    "def text_feature_pipe(field):\n",
    "    return Pipeline([\n",
    "        ('selector', FeatureSelector(field)),\n",
    "        ('processor', TextProcessor()),\n",
    "        ('tfidf', TfidfVectorizer()),])\n",
    "\n",
    "body = text_feature_pipe('text')\n",
    "adv = text_feature_pipe('advantages')\n",
    "dis = text_feature_pipe('disadvantages')\n",
    "\n",
    "emo = Pipeline([\n",
    "    ('selector', FeatureSelector('text')),\n",
    "    ('processor', TextProcessor()),\n",
    "    ('emo', EmoText()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "])\n",
    "\n",
    "\n",
    "text = FeatureUnion([\n",
    "    ('body', emo),\n",
    "    ('adv', adv),\n",
    "    ('dis', dis),\n",
    "#     ('emo', emo),\n",
    "])\n",
    "\n",
    "# ================== NUMERICAL FEATURES =============================\n",
    "\n",
    "text_stats = Pipeline([\n",
    "    ('stats', TextStats()),\n",
    "    ('vect', DictVectorizer(sparse=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "\n",
    "tone_features = Pipeline([\n",
    "    ('selector', FeatureSelector('text')),\n",
    "    ('processor', TextProcessor(lemma=True)),\n",
    "    ('tone', ToneFeature()),\n",
    "    ('vect', DictVectorizer(sparse=False)),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "author_features = Pipeline([\n",
    "    ('stats', AuthorType()),\n",
    "    ('vect', DictVectorizer(sparse=False)),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "numerical = FeatureUnion([\n",
    "    ('text_stats', text_stats),\n",
    "    ('tone', tone_features),\n",
    "    ('author', author_features),\n",
    "    ('like', FeatureSelector('like', to_values=True)),\n",
    "    ('dislike', FeatureSelector('dislike', to_values=True))\n",
    "])\n",
    "\n",
    "# ================= META-TEXT FEATURES ===============================\n",
    "\n",
    "\n",
    "bayes_text = Pipeline([\n",
    "    ('text', text),\n",
    "    ('rating', MetaFeature(MultinomialNB(), cv=5))\n",
    "])\n",
    "\n",
    "svc_text = Pipeline([\n",
    "    ('text', text),\n",
    "    ('rating', MetaFeature(SVC(C=10, probability=True), cv=5))\n",
    "])\n",
    "\n",
    "forest_text = Pipeline([\n",
    "    ('text', text),\n",
    "    ('rating', MetaFeature(RandomForestClassifier(n_estimators=100, class_weight='balanced'), cv=5))\n",
    "])\n",
    "\n",
    "# ============== META-NUMERICAL FEATURES =======================\n",
    "\n",
    "logit_numerical = Pipeline([\n",
    "    ('numerical', numerical),\n",
    "    ('rating', MetaFeature(LogisticRegression(), cv=5))\n",
    "])\n",
    "\n",
    "forest_numerical = Pipeline([\n",
    "    ('numerical', numerical),\n",
    "    ('rating', MetaFeature(RandomForestClassifier(n_estimators=100, max_depth=2, class_weight='balanced'), cv=5))\n",
    "])\n",
    "\n",
    "\n",
    "# ================ META-ESTIMATOR ============================\n",
    "\n",
    "features = FeatureUnion([\n",
    "    ('bayes', bayes_text),\n",
    "    ('svc', svc_text),\n",
    "    ('forest', forest_text),\n",
    "    ('logit_numerical', logit_numerical),\n",
    "    ('forest_numerical', forest_numerical),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('features', features),\n",
    "    ('logit', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'features__svc__text__body__processor__lemma': [False],\n",
    "    'features__svc__text__body__processor__no_stop_words': [True],\n",
    "    'features__svc__text__body__processor__clean': [True],\n",
    "    'features__svc__text__body__tfidf__ngram_range': [(1, 3)],\n",
    "    'features__svc__text__body__tfidf__max_df': [1.0],\n",
    "    \n",
    "    'features__svc__text__adv__processor__lemma': [True],\n",
    "    'features__svc__text__adv__processor__no_stop_words': [True],\n",
    "    'features__svc__text__adv__processor__clean': [True],\n",
    "    'features__svc__text__adv__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__svc__text__adv__tfidf__max_df': [0.7],\n",
    "    \n",
    "    'features__svc__text__dis__processor__lemma': [True],\n",
    "    'features__svc__text__dis__processor__no_stop_words': [True],\n",
    "    'features__svc__text__dis__processor__clean': [True],\n",
    "    'features__svc__text__dis__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__svc__text__dis__tfidf__max_df': [0.7],\n",
    "    \n",
    "    \n",
    "    'features__forest__text__body__processor__lemma': [False],\n",
    "    'features__forest__text__body__processor__no_stop_words': [True],\n",
    "    'features__forest__text__body__processor__clean': [True],\n",
    "    'features__forest__text__body__tfidf__ngram_range': [(1, 3)],\n",
    "    'features__forest__text__body__tfidf__max_df': [1.0],\n",
    "    \n",
    "    'features__forest__text__adv__processor__lemma': [True],\n",
    "    'features__forest__text__adv__processor__no_stop_words': [True],\n",
    "    'features__forest__text__adv__processor__clean': [True],\n",
    "    'features__forest__text__adv__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__forest__text__adv__tfidf__max_df': [0.7],\n",
    "    \n",
    "    'features__forest__text__dis__processor__lemma': [True],\n",
    "    'features__forest__text__dis__processor__no_stop_words': [True],\n",
    "    'features__forest__text__dis__processor__clean': [True],\n",
    "    'features__forest__text__dis__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__forest__text__dis__tfidf__max_df': [0.7],\n",
    "    \n",
    "    \n",
    "    'features__bayes__text__body__processor__lemma': [False],\n",
    "    'features__bayes__text__body__processor__no_stop_words': [True],\n",
    "    'features__bayes__text__body__processor__clean': [True],\n",
    "    'features__bayes__text__body__tfidf__ngram_range': [(1, 3)],\n",
    "    'features__bayes__text__body__tfidf__max_df': [1.0],\n",
    "    \n",
    "    'features__bayes__text__adv__processor__lemma': [True],\n",
    "    'features__bayes__text__adv__processor__no_stop_words': [True],\n",
    "    'features__bayes__text__adv__processor__clean': [True],\n",
    "    'features__bayes__text__adv__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__bayes__text__adv__tfidf__max_df': [0.7],\n",
    "    \n",
    "    'features__bayes__text__dis__processor__lemma': [True],\n",
    "    'features__bayes__text__dis__processor__no_stop_words': [True],\n",
    "    'features__bayes__text__dis__processor__clean': [True],\n",
    "    'features__bayes__text__dis__tfidf__ngram_range': [(1, 1)],\n",
    "    'features__bayes__text__dis__tfidf__max_df': [0.7],\n",
    "    \n",
    "#     'mnb__alpha': [0],\n",
    "#     'logit__penalty': ['l2'],\n",
    "#     'logit__solver': ['saga'],\n",
    "#     'logit__class_weight': ['balanced'],\n",
    "#     'logit__multi_class': ['ovr']\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(pipe, hyper_params, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in clf.get_params().keys():\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.610\n",
      "Mean Score [0.61014264]\n",
      "STD  [0.01002696]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.18      0.25        39\n",
      "           2       0.00      0.00      0.00        31\n",
      "           3       0.00      0.00      0.00        46\n",
      "           4       0.34      0.22      0.27       164\n",
      "           5       0.74      0.96      0.83       522\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       802\n",
      "   macro avg       0.30      0.27      0.27       802\n",
      "weighted avg       0.57      0.68      0.61       802\n",
      "\n",
      "CPU times: user 45 s, sys: 140 ms, total: 45.1 s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(data_train, y_train)\n",
    "\n",
    "print(f'Best Score: {clf.best_score_:.3f}')\n",
    "print('Mean Score', clf.cv_results_['mean_test_score'])\n",
    "print('STD ', clf.cv_results_['std_test_score'])\n",
    "print('\\n')\n",
    "\n",
    "y_predict = clf.predict(data_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   47.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   54.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   55.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.21      0.29        92\n",
      "           2       0.00      0.00      0.00        73\n",
      "           3       0.40      0.02      0.04       107\n",
      "           4       0.38      0.22      0.28       382\n",
      "           5       0.73      0.96      0.83      1215\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1869\n",
      "   macro avg       0.40      0.28      0.29      1869\n",
      "weighted avg       0.60      0.68      0.61      1869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = cross_val_predict(clf, data_train, y_train, cv=3)\n",
    "report = classification_report(y_train, pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc, nl = 0, 0\n",
    "for comment in jl_file(DATA_DIR / data_file):\n",
    "    if comment['lang'] == 'uk':\n",
    "        nc += 1\n",
    "    nl += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1684308065148437"
      ]
     },
     "execution_count": 1211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj-nlp-2019",
   "language": "python",
   "name": "prj-nlp-2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
